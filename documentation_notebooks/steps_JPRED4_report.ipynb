{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the Blind Set\n",
    "\n",
    "* Also known as **holdout dataset**\n",
    "\n",
    "* Cross-validation is not sufficient tot estimate unbiased generalization performance.\n",
    "    - Model hyper-parameters are still optimized on the training set through cross-valiation and grid-search\n",
    "    - This may lead to some degree of overfitting on training data\n",
    "    - Using a blind set helps us to generate a 'never-seen-before condition'\n",
    "    \n",
    "## Generation Criteria:\n",
    "\n",
    "* Structures deposited **after** January 2015\n",
    "     - Release year of JPred4 is 2014\n",
    "* X-ray crystals with resolution < 2,5 $\\overset{\\circ}{A}$\n",
    "* Chain lenght in the range of 50 - 300 residues\n",
    "* Advanced search -> Entry Polymer Types:\n",
    "    - Protein OR Protein/NA \n",
    "* All pairs of sequences within the blind set should share less than 30% sequence identity ('internal redundancy'):\n",
    "- By using ```blastclust``` we can reduce the redundancy\n",
    "\n",
    "* When comparing sequences of the blindset with the JPRED set: All pairs (blind - Jpred) have an less than 30% identity  ('external redundancy')\n",
    "- This will be ensured using ```blastp```\n",
    "\n",
    "* The final blind set will comprise 150 proteins which will be randomly selected among those that meet the above criteria\n",
    "\n",
    "### 1. Downloading Data from the PDB\n",
    "\n",
    "Checked the boxes \"Entry ID\",\"Sequence\",\"Entity Polymer Type\",\"Chain ID\",\"Entry Id (Polymer Entity Identifiers)\".\n",
    "\n",
    "[here the link to my search](https://www.rcsb.org/search?request=%7B%22query%22%3A%7B%22type%22%3A%22group%22%2C%22logical_operator%22%3A%22and%22%2C%22nodes%22%3A%5B%7B%22type%22%3A%22group%22%2C%22logical_operator%22%3A%22and%22%2C%22nodes%22%3A%5B%7B%22type%22%3A%22group%22%2C%22logical_operator%22%3A%22and%22%2C%22nodes%22%3A%5B%7B%22type%22%3A%22terminal%22%2C%22service%22%3A%22text%22%2C%22parameters%22%3A%7B%22operator%22%3A%22greater%22%2C%22negation%22%3Afalse%2C%22value%22%3A%222015-01-31T00%3A00%3A00Z%22%2C%22attribute%22%3A%22rcsb_accession_info.deposit_date%22%7D%2C%22node_id%22%3A0%7D%2C%7B%22type%22%3A%22terminal%22%2C%22service%22%3A%22text%22%2C%22parameters%22%3A%7B%22operator%22%3A%22exact_match%22%2C%22negation%22%3Afalse%2C%22value%22%3A%22X-RAY%20DIFFRACTION%22%2C%22attribute%22%3A%22exptl.method%22%7D%2C%22node_id%22%3A1%7D%2C%7B%22type%22%3A%22terminal%22%2C%22service%22%3A%22text%22%2C%22parameters%22%3A%7B%22operator%22%3A%22less_or_equal%22%2C%22negation%22%3Afalse%2C%22value%22%3A2.5%2C%22attribute%22%3A%22rcsb_entry_info.resolution_combined%22%7D%2C%22node_id%22%3A2%7D%2C%7B%22type%22%3A%22group%22%2C%22logical_operator%22%3A%22or%22%2C%22nodes%22%3A%5B%7B%22type%22%3A%22terminal%22%2C%22service%22%3A%22text%22%2C%22parameters%22%3A%7B%22operator%22%3A%22exact_match%22%2C%22negation%22%3Afalse%2C%22value%22%3A%22Protein%20(only)%22%2C%22attribute%22%3A%22rcsb_entry_info.selected_polymer_entity_types%22%7D%2C%22node_id%22%3A3%7D%2C%7B%22type%22%3A%22terminal%22%2C%22service%22%3A%22text%22%2C%22parameters%22%3A%7B%22operator%22%3A%22exact_match%22%2C%22negation%22%3Afalse%2C%22value%22%3A%22Protein%2FNA%22%2C%22attribute%22%3A%22rcsb_entry_info.selected_polymer_entity_types%22%7D%2C%22node_id%22%3A4%7D%5D%7D%2C%7B%22type%22%3A%22terminal%22%2C%22service%22%3A%22text%22%2C%22parameters%22%3A%7B%22operator%22%3A%22range_closed%22%2C%22negation%22%3Afalse%2C%22value%22%3A%5B50%2C300%5D%2C%22attribute%22%3A%22entity_poly.rcsb_sample_sequence_length%22%7D%2C%22node_id%22%3A5%7D%5D%7D%5D%2C%22label%22%3A%22text%22%7D%5D%2C%22label%22%3A%22query-builder%22%7D%2C%22return_type%22%3A%22entry%22%2C%22request_options%22%3A%7B%22pager%22%3A%7B%22start%22%3A0%2C%22rows%22%3A100%7D%2C%22scoring_strategy%22%3A%22combined%22%2C%22sort%22%3A%5B%7B%22sort_by%22%3A%22score%22%2C%22direction%22%3A%22desc%22%7D%5D%7D%2C%22request_info%22%3A%7B%22src%22%3A%22ui%22%2C%22query_id%22%3A%22dc19df09287d4c5a80018000a03e2a6d%22%7D%7D)\n",
    "\n",
    "* Downloaded as CSV \n",
    "\n",
    "For some reason it automatically adds \"Entry ID\" as column 1. Whenever there is another chain of the same ID the first line of col 1 will be empty --> removed it using awk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head -6 1.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in {1..5}\n",
    "do\n",
    "    cat ${i}.csv | awk '{sub(/[^,]*/,\"\");sub(/,/,\"\")} 1' > ${i}new.csv \n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head -5 4new.csv #now it looks like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parsing and Filtering\n",
    "\n",
    "First I need to be aware that some of the chains are nucleic acids (\"NA-hybrid\").\n",
    "\n",
    "Remove all lines containing\n",
    "*  \"NA-hybrid\" \n",
    "*  \"DNA\"\n",
    "*  \"RNA\"\n",
    "\n",
    "to obtain protein sequences only!\n",
    "\n",
    "```  sed -n '/Protein/p' ./filename \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all lines that do NOT contain \"Protein\" --> this removes the header too!\n",
    "head -1 1new.csv > aa_only.csv # Adding correct header to top of file that will be appended all \"Protein\" lines\n",
    "\n",
    "for i in {1..5} # Appending only lines containing word \"Protein\"\n",
    "do\n",
    "    sed -n '/Protein/p' ${i}new.csv >> aa_only.csv \n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head aa_only.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grep \"Protein\" aa_only.csv | wc -l #  29638 Protein chains in the set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switched to python 3 kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading relevant packages:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "# sns.set() #do we really need that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aa = pd.read_csv(\"aa_only.csv\")\n",
    "df_aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where did the unnamed come from??\n",
    "unnamed = df_aa[\"Unnamed: 4\"] # Maybe trailing comma?\n",
    "unnamed.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding unique values in cloumn \"Entity Polymer Type\"\n",
    "I want to find unique values as described [here](https://chrisalbon.com/python/data_wrangling/pandas_list_unique_values_in_column/)\n",
    "\n",
    "This way I can be sure that my cleaning was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding unique names in cols\n",
    "pol_type = df_aa['Entity Polymer Type']\n",
    "pol_type.unique() # I am now sure that all DNA and RNA and Protein/NA lines have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -3 aa_only.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed '1d' aa_only.csv > noheader_aa_only.csv # removing header before generating fasta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l aa_only.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l noheader_aa_only.csv # value matches file is ok proceeding to make fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I want to consider the comma as a field sepparator \n",
    "\n",
    "Since there are several chains per entry denoted as e.g.:\n",
    "\n",
    "``` \"CAGTTTCAAACTC\",\"D, I\",\"5FD3\",```\n",
    "\n",
    "I need to remove the comma between the chains first and replace it with a space:\n",
    "*  ``` sed 's/\\, / /g' $i.csv ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing commas between chains\n",
    "\n",
    "sed 's/\\, //g' aa_only.csv > nospace.csv\n",
    "\n",
    "head nospace.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generating the FASTA file\n",
    "\n",
    "* Using awk: defining the comma as field sepparator.\n",
    "\n",
    "* ``` awk -F ',' ```\n",
    "\n",
    "* filtering for lenght in the range of 50 - 300 \n",
    "\n",
    "*  ``` 'length($1) > 50 && length($1) < 301 {print \">\"$4\":\"$3\"\\n\"$1}' ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat nospace.csv | awk -F ',' 'length($1) > 50 && length($1) < 301 {print \">\"$4\":\"$3\"\\n\"$1}' | sed 's/\\\"//g' > 50_300.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head 50_300.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequences containing X need to be removed:\n",
    "\n",
    "Working on the script and testing it along the way. The final result is saved as\n",
    "\n",
    "#### removeX.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lines_to_list(filename):\n",
    "    ''' Reads all lines from a file and saves them to a list. '''\n",
    "    content_list = []\n",
    "    with open(filename, \"r\") as rfile:\n",
    "        content_list = rfile.readlines()\n",
    "        return content_list\n",
    "\n",
    "myfastalist = lines_to_list(\"50_300.fasta\")  #works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(myfastalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(liste):\n",
    "    ''' Splits a evennumbered list into two lists. id_list contains all odd items while seq_list contains all even items. Returns the two lists.'''\n",
    "    id_list = liste[::2]\n",
    "    seq_list = liste[1::2]\n",
    "    return id_list, seq_list\n",
    "\n",
    "# teste = ['a', 'b', 'c', 'd', 'e', 'f'] #Works\n",
    "ids, seq = split_list(myfastalist)\n",
    "\n",
    "\n",
    "print(len(ids))\n",
    "print(len(seq))\n",
    "print(len(myfastalist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_X(id1, seq2):\n",
    "    '''Removes items containing X in the sequence list but also the ID in the ID list. \n",
    "    Returns an ID list and an'''\n",
    "    noXid = []\n",
    "    noXseq = []\n",
    "    for i in range(len(id1)):\n",
    "        flag = \"X\" in seq2[i]\n",
    "        if flag == False:\n",
    "            noXid.append(id1[i])\n",
    "            noXseq.append(seq2[i])\n",
    "    return noXid, noXseq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_id, new_seq = remove_X(ids, seq)\n",
    "\n",
    "print(len(new_id))\n",
    "print(len(new_seq))\n",
    "\n",
    "def no_X_id_and_seq(id_list, seq_list):\n",
    "    ''' Joins the lists to a big list containing both id and sequences. Returns one big list'''\n",
    "    biglist = []\n",
    "    for i in range(len(id_list)):\n",
    "        biglist.append(id_list[i])\n",
    "        biglist.append(seq_list[i])\n",
    "    return biglist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biglist = no_X_id_and_seq(new_id, new_seq)\n",
    "len(biglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing list to file:\n",
    "\n",
    "def list_to_fasta(liste):\n",
    "    '''Takes one id list and one sequlist as input. Writes all elements i to \n",
    "    a file. Returns the file.'''\n",
    "    with open('no_X.fasta', 'w') as F:\n",
    "        for i in liste:\n",
    "            F.write(str(i))\n",
    "    F.close            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_fasta(biglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short(infile, outfile):\n",
    "    del_seq_index = []\n",
    "    lines_list = []\n",
    "    with open(infile) as rfile:\n",
    "        lines_list = rfile.readlines()\n",
    "        for i in range(1, len(lines_list),2):\n",
    "            if len(lines_list[i]) < 7:\n",
    "                del_seq_index.append(i-1) # appending header index\n",
    "                del_seq_index.append(i)   # appending sequence index\n",
    "    with open(outfile, 'w') as wfile:\n",
    "        for i in range(len(lines_list)):\n",
    "            if i in del_seq_index:\n",
    "                continue\n",
    "            wfile.write(lines_list[i])    \n",
    "        \n",
    "filter_short(\"no_X.fasta\", 'longsequences.fasta')                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Clustering the Sequences in blastclust\n",
    "\n",
    "Sending file to be clustered to the VM:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scp no_X_long_sequs.fasta @proj:~/lb2-2020-project-englander\n",
    "\n",
    "screen\n",
    "\n",
    "source /opt/conda/bin/activate\n",
    "\n",
    "blastclust -i no_X_long_sequs.fasta -o final_clusters -S 30 -L 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Picking longest seuqence of each cluster: by default col1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cat final_clusters | awk $1 {print} > best_of_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in {1..5}\n",
    "do\n",
    "    mv $i.csv ./orignial_csv/$i.csv\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in {1..5}\n",
    "do\n",
    "     mv ${i}new.csv ./orignial_csv/${i}new.csv\n",
    "done     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Generating FASTA Containing ONLY Sequences of Best Cluster\n",
    "\n",
    "* I think the easiest is if I generate a list of best_of_final_cluster\n",
    "\n",
    "    - Need to first generate a new file that contains the \">\" character in front of every sequence\n",
    "\n",
    "\n",
    "* And generate a dictionary of the no_X_long_sequs.fasta   \n",
    "\n",
    "* Then I want to use the list to loop on the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure the output of the script will be fasta standard\n",
    "cat best_of_final_cluster | sed 's/^/>/' > crocodile_ids_final_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "def lines_to_list(infile1): \n",
    "    ''' Reads all lines from a file and saves them to a list. '''\n",
    "    content_list = []\n",
    "    with open(infile1, \"r\") as rfile:\n",
    "        content_list = rfile.readlines()\n",
    "        return content_list\n",
    "\n",
    "def split_list(infile1):\n",
    "    ''' Splits a evennumbered list into two lists. id_list contains \n",
    "    all odd items while seq_list contains all even items. Returns the two lists.'''\n",
    "    myfastalist = lines_to_list(infile1)  #works\n",
    "    id_list = myfastalist[::2]\n",
    "    seq_list = myfastalist[1::2]\n",
    "    return id_list, seq_list\n",
    "\n",
    "def dict_from_lists(infile1):\n",
    "    '''Takes feeds two lists into a dictionary. \n",
    "    Returns the dicitonary'''\n",
    "    id_list, seq_list = split_list(infile1)\n",
    "    keys = id_list\n",
    "    values = seq_list\n",
    "    full_dict = dict(zip(keys, values))\n",
    "    return full_dict\n",
    "    \n",
    "def keep_whats_in_dict(infile1, infile2, outfile):\n",
    "    '''Loops through a list and a dictionary. Appending the values\n",
    "    of the list (PDB ids which are also the keys of the dictionary) and the\n",
    "    values of the dictionary to the outfile.'''\n",
    "    idlist = lines_to_list(infile2) # reading ids from file into list\n",
    "    aa_dict = dict_from_lists(infile1)\n",
    "    with open(outfile, 'a') as afile:\n",
    "        for i in idlist:\n",
    "            afile.write(i) #appending ID in even lines\n",
    "            afile.write(aa_dict[i]) # appending value (sequ) in odd lines\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    infile1 = sys.argv[1]\n",
    "    infile2 = sys.argv[2]\n",
    "    outfile = sys.argv[3]\n",
    "    keep_whats_in_dict(infile1, infile2, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the script\n",
    "python  make_fasta_from_best_of_each_cluster.py no_X_long_sequs.fasta crocodile_ids_final_cluster crocodile_best_of_final_cluster.fasta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scp -i ~/.ssh/id_rsa.pub ./make_fasta_from_best_of_each_cluster.py proj:~/lb2-2020-project-englander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scp -i ~/.ssh/id_rsa.pub ./crocodile_best_of_final_cluster.fasta proj:~/lb2-2020-project-englander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Mergeing all fasta files of the jpred (training) set \n",
    "\n",
    "Need it later to generate blastdb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat *.fasta > JPREDall.fasta  # merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grep \">\" JPREDall.fasta | wc -l  # works --> merged all 1348 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scp -i ~/.ssh/id_rsa.pub ./JPREDall.fasta proj:~/lb2-2020-project-englander"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "makeblastdb -in ./makeblastdb/JPREDall.fasta -dbtype prot\n",
    "\n",
    "Building a new DB, current time: 09/23/2020 14:29:12\n",
    "New DB name:   /home/um19/lb2-2020-project-englander/makeblastdb/JPREDall.fasta\n",
    "New DB title:  ./makeblastdb/JPREDall.fasta\n",
    "Sequence type: Protein\n",
    "Keep MBits: T\n",
    "Maximum file size: 1000000000B\n",
    "Adding sequences from FASTA; added 1348 sequences in 0.06898 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Reducing Redundancy\n",
    "\n",
    "I want to produce a blind testset that is as dissimilar to the training set as possible.\n",
    "\n",
    "* Running blastp with blindset against JPRED training set\n",
    "* I dentifying all sequences in the blindset that have LESS than 30% seq ID with any other sequ in the training set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " blastp -query ../crocodile_best_of_final_cluster.fasta -db JPREDall.fasta -evalue 0.01 -out hits.blastp.tab -outfmt 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying hits.blast.tab to local\n",
    "scp -i ~/.ssh/um19_id_rsa um19@m19.lsb.biocomp.unibo.it:~/lb2-2020-project-englander/makeblastdb/hits.blastp.tab ~/01-Unibo/02_Lab2/project_blindset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc -l hits.blastp.tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head -4 hits.blastp.tab\n",
    "# $1 pdb_Id $2 jprd id $3 % identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Non-Redundant Set With Least Similarty \n",
    "\n",
    "Step 3 from the Slides: \"Filter out from the preliminary chain set, all chains having at least one BLAST hit with SI >= 30% with any sequence in the JPRED4 dataset\n",
    "\n",
    "### Generating 2 Files Containing Only Relevant Lines\n",
    "\n",
    "file above30:\n",
    "* I'll extract ```$1``` if ``` $3 > 30 ```\n",
    "\n",
    "file below30\n",
    "* I have to extract ```$1``` if col ```$3 < 30 ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# awk keep filed if col 3 < 30 pipe to new file.\n",
    "awk -F ' ' '$3 < 30 {print $1 \" \" $3}' hits.blastp.tab > below_30.hits\n",
    "awk -F ' ' '$3 >= 30 {print $1 \" \" $3}' hits.blastp.tab > above_30.hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keeping IDs only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awk -F ' ' '$3 < 30 {print $1}' hits.blastp.tab > id_below_30.hits\n",
    "awk -F ' ' '$3 >= 30 {print $1}' hits.blastp.tab > id_above_30.hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lines_to_list(infile1): \n",
    "    ''' Reads all lines from a file and saves them to a list. '''\n",
    "    content_list = []\n",
    "    with open(infile1, \"r\") as rfile:\n",
    "        content_list = rfile.readlines()\n",
    "        return content_list\n",
    "        \n",
    "below = lines_to_list(\"id_below_30.hits\")  # list of all ids scoring below 30% id\n",
    "above = lines_to_list('id_above_30.hits')  # list of all ids scoring above and equal to 30% id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(below))\n",
    "print(len(above))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"6D8X:A\\n\" in above\n",
    "# head -30 id_below_30.hits > ids_test\n",
    "# head -30 id_above_30.hits > ids_above_test\n",
    "\n",
    "def remove_matches(lower, higher):\n",
    "    '''Takes two lists as input and returns a list that contains\n",
    "    all values of 'lower' values that are NOT element of 'higher'.'''\n",
    "    keepers = []          # list holding all ids that have not scored >= 30% with any of the JPRED sequnces\n",
    "    for i in lower:       \n",
    "        if i not in higher: \n",
    "            keepers.append(i)  # keeps only ids that are not reported in the list \"above\"\n",
    "    return keepers\n",
    "\n",
    "keep = remove_matches(below, above) \n",
    "\n",
    "print(len(keep))\n",
    "# Have 177 unique sequnces with no match above 30% id with any other sequ in the testing (JPRED) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = lines_to_list(\"best_of_final_cluster\") # generating list of all ids that were in the blastp input\n",
    "print(len(all_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_mis_matches(biglist, partiallist):\n",
    "    '''Stores exclusively biglist values that are not reported in partiallist.\n",
    "    Returns the biglist with all partiallist matches removed. Keeps all values \n",
    "    that donot match any element of partiallist in a new list. Returns the new list.'''\n",
    "    keepers = []\n",
    "    for i in biglist:\n",
    "        if i not in partiallist:\n",
    "            keepers.append(i) \n",
    "    return keepers\n",
    "\n",
    "all_without_above = keep_mis_matches(all_ids, above)\n",
    "len(all_without_above)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_to_file(liste, newfile):\n",
    "    '''Takes as input a list and writes each element to a new file'''\n",
    "    with open(newfile, 'a') as afile: \n",
    "        for i in liste:\n",
    "            afile.write(i)\n",
    "            \n",
    "write_list_to_file(all_without_above, 'try_again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_scoring_below30.py\n",
    "\n",
    "# input of blastp = fastafile find missing\n",
    "\n",
    "# interseciton of set below() and above() --> throw_away\n",
    "# find and remove throw away id file\n",
    "\n",
    "# make list of all ids reproted in fastinputblastp\n",
    "# turn fastinputblastp_list into fastinputblastp_set (biggest set)\n",
    "# fastinputblastp_set - above_set \n",
    "\n",
    "def lines_to_list(infile1): \n",
    "    ''' Reads all lines from a file and saves them to a list. '''\n",
    "    content_list = []\n",
    "    with open(infile1, \"r\") as rfile:\n",
    "        content_list = rfile.readlines()\n",
    "        return content_list\n",
    "        \n",
    "below = lines_to_list(\"id_below_30.hits\")  # list of all ids scoring below 30% id\n",
    "above = lines_to_list('id_above_30.hits')  # list of all ids scoring above and equal to 30% id\n",
    "\n",
    "def remove_matches(lower, higher):\n",
    "    '''Takes two lists as input and returns a list that contains\n",
    "    all values of 'lower' values that are NOT element of 'higher'.'''\n",
    "    keepers = []          # list holding all ids that have not scored >= 30% with any of the JPRED sequnces\n",
    "    for i in lower:       \n",
    "        if i not in higher: \n",
    "            keepers.append(i)  # keeps only ids that are not reported in the list \"above\"\n",
    "    return keepers\n",
    "\n",
    "keep = remove_matches(below, above) \n",
    "\n",
    "# print(len(keep))\n",
    "# Have 177 unique sequnces with no match above 30% id with any other sequ in the testing (JPRED) set.\n",
    "\n",
    "##########################################################\n",
    "# Making a list of all ids (input IDs of the blastp)\n",
    "##########################################################\n",
    "\n",
    "all_ids = lines_to_list(\"best_of_final_cluster\") # generating list of all ids that were in the blastp input\n",
    "# print(len(all_ids))\n",
    "\n",
    "def keep_mis_matches(biglist, partiallist):\n",
    "    '''Stores exclusively biglist values that are not reported in partiallist.\n",
    "    Returns the biglist with all partiallist matches removed. Keeps all values \n",
    "    that donot match any element of partiallist in a new list. Returns the new list.'''\n",
    "    keepers = []\n",
    "    for i in biglist:\n",
    "        if i not in partiallist:\n",
    "            keepers.append(i) \n",
    "    return keepers\n",
    "\n",
    "all_without_above = keep_mis_matches(all_ids, above)\n",
    "# len(all_without_above)\n",
    "\n",
    "def write_list_to_file(liste, newfile):\n",
    "    '''Takes as input a list and writes each element to a new file'''\n",
    "    with open(newfile, 'a') as afile: \n",
    "        for i in liste:\n",
    "            afile.write(i)\n",
    "            \n",
    "write_list_to_file(all_without_above, 'try_again')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly Sort and Pick 150 Sequences \n",
    "\n",
    "Sometimes not all IDS have an associated PDB file thus I select 160 to be sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ids_0-30 | sort -R | head -160 > random.blindset2 # in case some PDB files are not available\n",
    "!wc -l random.blindset2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I still have *NOT* removed identical chain IDs:\n",
    "\n",
    "* The file still contains all letters denoting different chains after the \":\"\n",
    "\n",
    "* printing it to a new file only keeping first letter after the \":\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail random.blindset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat random.blindset2 | cut -c-6 > id_and_chain_blindset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if really no above 30 are in the new list\n",
    "def lines_to_list(infile1): \n",
    "    ''' Reads all lines from a file and saves them to a list. '''\n",
    "    content_list = []\n",
    "    with open(infile1, \"r\") as rfile:\n",
    "        content_list = rfile.readlines()\n",
    "        return content_list\n",
    "        \n",
    "bigset = lines_to_list(\"random.blindset2\")  # list of all ids scoring below 30% id\n",
    "above = lines_to_list('id_above_30.hits')  # list of all ids scoring above and equal to 30% id\n",
    "\n",
    "def remove_matches(l1, l2):\n",
    "    '''Takes two lists as input and returns a list that contains\n",
    "    all values of 'lower' values that are NOT element of 'higher'.'''\n",
    "    keepers = []          # list holding all ids that have not scored >= 30% with any of the JPRED sequnces\n",
    "    for i in l1:       \n",
    "        if i not in l2: \n",
    "            keepers.append(i)  # keeps only ids that are not reported in the list \"above\"\n",
    "    return keepers\n",
    "\n",
    "keep = remove_matches(bigset, above) \n",
    "len(keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Download Structures from PDB\n",
    "\n",
    "For each of the 150 sequences I have to download the pdb structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat random.blindset2 | cut -c-4 > only_id_blindset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in\n",
    "# https://files.rcsb.org/view/6OZJ.pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Generate DSSP files from selected PDB files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lines_to_list(infile1): \n",
    "    ''' Reads all lines from a file and saves them to a list. '''\n",
    "    content_list = []\n",
    "    with open(infile1, \"r\") as rfile:\n",
    "        content_list = rfile.readlines()\n",
    "        return content_list\n",
    "        \n",
    "bigset = lines_to_list(\"only_id_blindset2\") # Contain trailing newline char\n",
    "#removing \\n :\n",
    "def rstrip_each_item(list):\n",
    "    my_150_PDB = []\n",
    "    for i in list:\n",
    "        clean = i.rstrip()\n",
    "        my_150_PDB.append(clean)\n",
    "    return my_150_PDB\n",
    "\n",
    "my_150_PDB = rstrip_each_item(bigset)\n",
    "print(my_150_PDB)\n",
    "len(my_150_PDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat only_id_blindset2 | sed -z 's/\\n/, /g' > commas2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd blindset_all_PDBs/150_blind_PDBs/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in {1..150}\n",
    "do\n",
    "    gunzip *ent.gz\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Generating DSSP files from all 150 PDB files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "# Running the DSSP on the extracted PDB files\n",
    "for i in *.ent\n",
    "do\n",
    "        mkdssp -i \"$i\" -o \"./dsspout/$i.dssp\"\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " scp -i ~/.ssh/id_rsa.pub ./150_blind_PDBs/* proj:~/lb2-2020-project-englander/150_blind_PDBs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Desired Chain From Each DSSP file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting chain and secondary structure from DSSP files:\n",
    "\n",
    "* protein chain: ```$3```\n",
    "* Secondary Structure Summanry ```$4```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls id_and_chain_blindset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /Users/ila/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ila/01-Unibo/02_Lab2/project_blindset'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewinding Undoing Some Mess\n",
    "Forgot that I downloaded 160 structures --> since not all were available in PDB format I just took a little extra. Then I only unzipped 150. But which did I NOT use? \n",
    "Who knows - sloppy documentation -_-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4Y4O,4YBB,4Y4O,5XJL,6EHA,6NTV,6T8S,4YWN,5FLY,5JVV,"
     ]
    }
   ],
   "source": [
    "def lines_list(fname):\n",
    "    with open(fname) as ofile:\n",
    "        flist = ofile.readlines() # returns list containing each line of the file\n",
    "        return flist\n",
    "\n",
    "dsspinfo = lines_list(\"/Users/ila/01-Unibo/02_Lab2/project_blindset/deletechainmess/dssp_file_names\")  # generating list of all file names e.g. \"pdb7jtl.ent.dssp\"\n",
    "\n",
    "def caps_dssp_list(li): \n",
    "    '''Takes as input raw DSSP list and returns the pdb IDs of each file in a list.'''\n",
    "    dssp_id_list =[]\n",
    "    for i in range(len(li)):  \n",
    "        el = dsspinfo[i]             # isolating list el\n",
    "        new_el = el[3:7].upper()     # creating new_el wich is only the fileds containing the PDB id\n",
    "        dssp_id_list.append(new_el+\"\\n\")  # append each id to dssp_list\n",
    "    return dssp_id_list\n",
    "\n",
    "dssp_ids = caps_dssp_list(dsspinfo)\n",
    "\n",
    "pdb_ids_160 =lines_list(\"only_id_blindset2\")\n",
    "\n",
    "# print(pdb_ids_160)\n",
    "\n",
    "def keep_matches(biglist, partiallist):\n",
    "    '''Stores all common values that are reported in both lists.\n",
    "    Returns the intersection of items as keepers list.'''\n",
    "    keepers = []\n",
    "    for i in biglist:\n",
    "        if i in partiallist:\n",
    "            keepers.append(i) \n",
    "    return keepers\n",
    "\n",
    "all150dssp = keep_matches(pdb_ids_160, dssp_ids)\n",
    "# print(all150dssp)\n",
    "\n",
    "def remove_matches(l1, l2):\n",
    "    '''Takes two lists as input and returns a list that contains\n",
    "    all values of 'lower' values that are NOT element of 'higher'.'''\n",
    "    keepers = []          # list holding all ids that have not scored >= 30% with any of the JPRED sequnces\n",
    "    for i in l1:       \n",
    "        if i not in l2: \n",
    "            keepers.append(i)  # keeps only ids that are not reported in the list \"above\"\n",
    "    return keepers\n",
    "\n",
    "keep = remove_matches(pdb_ids_160, dssp_ids) \n",
    "\n",
    "for i in keep:\n",
    "    print(i.rstrip()+\",\", end=\"\") # deleted them manually  -_- brainfog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unused:\n",
    "From the origninal download I did NOT use the following PDB ids:\n",
    "   * 4Y4O,4YBB,4Y4O,5XJL,6EHA,6NTV,6T8S,4YWN,5FLY,5JVV,\n",
    "   * saved them to ```~/lb2-2020-project-englander/unused_10```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdb4ywn.ent                                   100%  395KB 930.2KB/s   00:00    \n",
      "pdb5xjl.ent                                   100%  441KB 800.6KB/s   00:00    \n",
      "pdb5fly.ent                                   100% 1257KB   1.2MB/s   00:01    \n",
      "pdb5jvv.ent                                   100%  880KB 947.6KB/s   00:00    \n",
      "pdb6ntv.ent                                   100%  481KB   1.0MB/s   00:00    \n",
      "pdb6eha.ent                                   100%  374KB   1.2MB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "#sending them to the VM for safekeeping (incase I need to replace any of the files generated)\n",
    "# the -r argument --> recursively copy these files to the VM\n",
    "scp -i ~/.ssh/id_rsa.pub -r ~/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/unused_10/ proj:~/lb2-2020-project-englander/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdb4ywn.ent.dssp                              100%   41KB 414.3KB/s   00:00    \n",
      "pdb6eha.ent.dssp                              100%   61KB 565.7KB/s   00:00    \n",
      "pdb6ntv.ent.dssp                              100%   95KB 411.9KB/s   00:00    \n",
      "pdb5fly.ent.dssp                              100%   78KB 565.8KB/s   00:00    \n",
      "pdb5xjl.ent.dssp                              100%   86KB 913.9KB/s   00:00    \n",
      "pdb5jvv.ent.dssp                              100%   83KB 563.4KB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "# ran dssp sepperately on these files to have some backup inscase some dont have what they need\n",
    "scp -i ~/.ssh/um19_id_rsa -r um19@m19.lsb.biocomp.unibo.it:~/lb2-2020-project-englander/unused_10/dsspout_unused10/ ~/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/unused_10/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The function\n",
    "\n",
    "# extract_ss_from_dssp_using_2input_files.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lines_list(infile1):                                              # call list of file names and for dsspfile\n",
    "    ''' Reads all lines from a file and saves them to a list. '''\n",
    "    with open(infile1) as ofile:\n",
    "        flist = ofile.readlines() # returns list containing each line of the file\n",
    "        return flist\n",
    "\n",
    "def relevant_lines(infile1, desired_chain):\n",
    "    '''Takes list (extracted from a DSSP file) and the name of the desired_chain as input.\n",
    "    Returns 2 strings: ss_string holds the secondary structure mapping and aa_string holds \n",
    "    the amino acid information. Missing residues (when no atomic information of the PDB is \n",
    "    present) are assigned the letter \"C\" (coil) in the ss_string and \"X\" in the aa_string.'''\n",
    "    dssp_list = lines_list(infile1)     # contains all lines from the dssp file.\n",
    "    relevant = False # boolean variable            \n",
    "#     desired_chain = \"A\"                            # change to load from \"id_and_chain_blindset2\"\n",
    "    ss_string = ''\n",
    "    aa_string = ''\n",
    "    for line in dssp_list:\n",
    "        if '#' in line: # find last line before relevant output\n",
    "            relevant =True   # flips rel to true - so the folowing lines are saved\n",
    "            continue\n",
    "        if relevant:\n",
    "            if line[11] == desired_chain:\n",
    "                ss_string += line[16]\n",
    "                if line[13] == \"!\":\n",
    "                    aa_string += \"X\"\n",
    "                else:\n",
    "                    aa_string += line[13]\n",
    "    return ss_string, aa_string\n",
    "\n",
    "def raw_to_threclasses(rawstring):\n",
    "        structure_dict = {\"H\":\"H\", \"G\":\"H\", \"I\":\"H\", \"B\":\"E\", \"E\":\"E\", \"T\":\"C\", \"S\":\"C\", \" \":\"C\"} \n",
    "        threeclasses = ''\n",
    "        for letter in rawstring:\n",
    "                threeclasses += structure_dict[letter]\n",
    "        return threeclasses    \n",
    "    \n",
    "def generate_dssp_fasta(filename_id, chain): \n",
    "    '''Writes SS to dsspfile and AA to fastafile'''\n",
    "    # reads dssp file and returns ss_string and aa_string.\n",
    "    ss_string, aa_string = relevant_lines(\"/Users/ila/01-Unibo/02_Lab2/project_blindset/deletechainmess/testruns/pdb\"+filename_id+\".ent.dssp\", chain)     \n",
    "    with open(filename_id+\".dssp\", 'w') as dsspfile:\n",
    "        ss = raw_to_threclasses(ss_string)\n",
    "        dsspfile.write(\">\"+filename_id+\"_\"+chain+\"\\n\")\n",
    "        dsspfile.write(ss+\"\\n\")\n",
    "        \n",
    "    with open(filename_id+\".fasta\", 'w') as fastafile:    \n",
    "        fastafile.write(\">\"+filename_id+\"_\"+chain+\"\\n\")\n",
    "        fastafile.write(aa_string+\"\\n\")\n",
    "        if aa_string == \"\":\n",
    "                print('empty',filename_id, chain, end=\"\")\n",
    "        \n",
    "# creating list holding all pdb ids and chain descriptions        \n",
    "id_chain = lines_list('/Users/ila/01-Unibo/02_Lab2/project_blindset/deletechainmess/BLINDset_id_and_chain')  \n",
    "\n",
    "for el in id_chain:            # each el is like \"6LTZ:A\"\n",
    "    field_list = el.split(':') # list  BOTH contains ID [0] and chain [1]\n",
    "    fname_id =  field_list[0].lower()                   #\n",
    "#     fname = \"pdb\"+field_list[0].lower()+\".ent.dssp\"     #fname to be used in the function calls\n",
    "    chain = field_list[1].rstrip()                             # chain name to be used in each function call\n",
    "    generate_dssp_fasta(fname_id, chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6LTZ:A\\n', '6EXX:A\\n', '5XGA:A\\n', '5WUJ:B\\n', '4ZY7:A\\n', '5F2A:A\\n', '5D1R:A\\n', '5UNI:A\\n', '5U7E:A\\n', '5ANP:A\\n', '6HKS:A\\n', '6J0Y:C\\n', '6L77:A\\n', '6KKO:A\\n', '6GW6:B\\n', '5LDD:B\\n', '6K7Q:A\\n', '4Y0L:A\\n', '5GNA:B\\n', '5C8A:A\\n', '4ZC4:A\\n', '6EI6:A\\n', '5UMV:A\\n', '6OR3:A\\n', '5U4U:A\\n', '5XKS:A\\n', '5GHL:A\\n', '5XYF:A\\n', '5N07:A\\n', '5FB9:A\\n', '5CEG:A\\n', '7BWF:B\\n', '4YTE:A\\n', '5V0M:A\\n', '6SE1:A\\n', '5UC0:A\\n', '5WNW:A\\n', '5IHF:A\\n', '5T2Y:A\\n', '5IB0:A\\n', '6USC:A\\n', '4UIQ:A\\n', '6YJ1:A\\n', '6DHX:A\\n', '5D6T:A\\n', '6DN4:A\\n', '5BP5:C\\n', '6ISU:A\\n', '6FSF:A\\n', '5VOG:A\\n', '5IR2:A\\n', '5D71:A\\n', '5BPX:A\\n', '5II0:A\\n', '4Y0O:A\\n', '5AUN:A\\n', '5C5Z:A\\n', '5KWV:A\\n', '6MDW:A\\n', '5FQ0:A\\n', '7BVV:A\\n', '5AV5:A\\n', '5FFL:A\\n', '6OOD:A\\n', '5KQA:A\\n', '5DCF:A\\n', '5GKE:A\\n', '5ZRY:A\\n', '5UIV:A\\n', '6GBI:A\\n', '5MC9:B\\n', '6FWT:A\\n', '6HSV:A\\n', '6HFG:B\\n', '5A88:A\\n', '5YEI:B\\n', '6NDR:A\\n', '5KLC:A\\n', '6MAB:A\\n', '6AOZ:A\\n', '5T2X:A\\n', '5LTF:A\\n', '6J19:B\\n', '6T7O:A\\n', '6MLX:A\\n', '5YMX:A\\n', '7JTL:A\\n', '5K21:A\\n', '5CTD:C\\n', '6R5W:A\\n', '5EIV:A\\n', '5HT8:A\\n', '5DD8:A\\n', '5D16:A\\n', '4ZEY:A\\n', '6VCI:A\\n', '5Y7W:A\\n', '4ZLR:A\\n', '5XVK:A\\n', '6VK4:D\\n', '6IA7:A\\n', '5HJF:A\\n', '6QLC:A\\n', '6WK3:A\\n', '5WOQ:A\\n', '5YSN:B\\n', '5Z1N:A\\n', '5BPU:A\\n', '5ABR:A\\n', '4ZKP:A\\n', '6ON1:A\\n', '6I9L:A\\n', '5AZW:A\\n', '6KOK:A\\n', '5Y4R:C\\n', '5CYB:A\\n', '5A6W:C\\n', '5U5N:A\\n', '5BN2:A\\n', '6UXF:A\\n', '6K6L:A\\n', '5WD6:A\\n', '6OVI:A\\n', '5V2I:A\\n', '6IQO:A\\n', '6MD3:A\\n', '5JSN:B\\n', '5JWO:A\\n', '6Q7N:A\\n', '5NL9:A\\n', '6Q8J:A\\n', '5WD8:A\\n', '6CEQ:A\\n', '5BPK:C\\n', '5BXQ:A\\n', '6OKM:R\\n', '5MMH:A\\n', '6H9E:A\\n', '5F1S:A\\n', '6SLK:B\\n', '5DG6:A\\n', '5DQ0:A\\n', '5X4B:A\\n', '5B71:E\\n', '4ZRZ:A\\n', '5M9O:A\\n', '6DEW:A\\n', '5U39:A\\n', '6R82:A\\n', '6G3Z:A\\n']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def lines_list(infile1):                                              # call list of file names and for dsspfile\n",
    "    ''' Reads all lines from a file and saves them to a list. '''\n",
    "    with open(infile1) as ofile:\n",
    "        flist = ofile.readlines() # returns list containing each line of the file\n",
    "        return flist\n",
    "    \n",
    "def findX(filename):\n",
    "    with open(filename+\".fasta\") as myfasta:\n",
    "        id = myfasta.readline()\n",
    "        sequ = myfasta.readline()\n",
    "        if 'X' in sequ == True:\n",
    "            print(filename, sequ)\n",
    "        else:\n",
    "            print(\"no X\")\n",
    "\n",
    "id_chain = lines_list('/Users/ila/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/BLINDset_id_and_chain')              \n",
    "\n",
    "for el in id_chain:            # each el is like \"6LTZ:A\"\n",
    "    field_list = el.split(':') # list  BOTH contains ID [0] and chain [1]\n",
    "    fname_id =  field_list[0].lower()                   #\n",
    "#     fname = \"pdb\"+field_list[0].lower()+\".ent.dssp\"     #fname to be used in the function calls\n",
    "    chain = field_list[1].rstrip()                             # chain name to be used in each function call\n",
    "    findX(fname_id)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6j19.fasta less than 50\n"
     ]
    }
   ],
   "source": [
    "def lines_list(infile1):                                              # call list of file names and for dsspfile\n",
    "    ''' Reads all lines from a file and saves them to a list. '''\n",
    "    with open(infile1) as ofile:\n",
    "        flist = ofile.readlines() # returns list containing each line of the file\n",
    "        return flist\n",
    "    \n",
    "def count_len_seq(filename):\n",
    "    with open('/Users/ila/01-Unibo/02_Lab2/project_blindset/blind_fasta/'+filename) as myfasta:\n",
    "        id = myfasta.readline()\n",
    "        sequ = myfasta.readline()\n",
    "        lseq = len(sequ)\n",
    "        if lseq <= 50:\n",
    "            print(filename, \"less than 50\")\n",
    "        if lseq >= 300:\n",
    "            print(filename, 'more than 300')\n",
    "\n",
    "id_chain = lines_list('/Users/ila/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/BLINDset_id_and_chain')              \n",
    "\n",
    "for el in id_chain:            # each el is like \"6LTZ:A\"\n",
    "    field_list = el.split(':') # list  BOTH contains ID [0] and chain [1]\n",
    "    fname_id =  field_list[0].lower()\n",
    "#     fname = \"pdb\"+field_list[0].lower()+\".ent.dssp\"     #fname to be used in the function calls\n",
    "    chain = field_list[1].rstrip()                             # chain name to be used in each function call\n",
    "    count_len_seq(fname_id+'.fasta')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing 6j19.fasta as it has < 50 aa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing from \"unused set\" in\n",
    "``` ~/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/unused_10/unused_10_fasta```\n",
    "\n",
    "List of unused sequs and chains\n",
    "```['5XJL:2', '6EHA:A', '6NTV:A', '4YWN:A', '5FLY:A', '5FLY:A']```\n",
    "\n",
    "replacing 9j19.fasta and dssp with 4ywn_A -- to set of 150 sequs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ila/01-Unibo/02_Lab2/project_blindset\n"
     ]
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv /Users/ila/01-Unibo/02_Lab2/project_blindset/blind_dssp/6j19.dssp ~/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/unused_10/unused_9_dssp/ \n",
    "mv /Users/ila/01-Unibo/02_Lab2/project_blindset/blind_fasta/6j19.fasta ~/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/unused_10/unused_10_fasta/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4ywn.fasta\n",
      "5fly.fasta\n",
      "5xjl.fasta\n",
      "6eha.fasta\n",
      "6j19.fasta\n",
      "6ntv.fasta\n"
     ]
    }
   ],
   "source": [
    "ls -1 ~/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/unused_10/unused_10_fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv ~/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/unused_10/unused_9_dssp/4ywn.dssp /Users/ila/01-Unibo/02_Lab2/project_blindset/blind_dssp/\n",
    "mv ~/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/unused_10/unused_10_fasta/4ywn.fasta /Users/ila/01-Unibo/02_Lab2/project_blindset/blind_fasta/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected the mistake have now 150 dssp and fasta of sufficient lenght\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making sure I really have no lower case letters in the sequence:\n",
    "a = \"AaAA\"\n",
    "a.islower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just checking if I have any lower case letters indicating C-C bridges in any of the fastafiles of my blindset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4uiq\n",
      "4y0l\n",
      "4y0o\n",
      "4yte\n",
      "4ywn\n",
      "4zc4\n",
      "4zey\n",
      "4zkp\n",
      "4zlr\n",
      "4zrz\n"
     ]
    }
   ],
   "source": [
    "!head list_of_blindset_ids_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``` project_blindset/scripts/lowercase_infile.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/anaconda3/bin/python\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# script to check if there are any lowercase letters corresponding to SS bridges (Cysteins).\n",
    "def find_lowerletters(infile1):\n",
    "    ''' \n",
    "    Reads lines from a file  EXCLUDING line 1. Saves string to a variable.\n",
    "    Checks each character. If character is lower case it is replaced by a C. As all dssp files designate\n",
    "    SS-bridges with lower case pairs.\n",
    "    '''\n",
    "    with open(infile1) as ofile:\n",
    "        lines = ofile.readlines()\n",
    "        header = lines[0]\n",
    "        sequence = lines[1] # list of each line of the file excluding line 0  \n",
    "        upper_seq = ''\n",
    "    # check_string = lines_list(infile1)[0] # l[0] to check_string\n",
    "    for char in sequence:\n",
    "        if char.isupper():\n",
    "            upper_seq += char\n",
    "        if char.islower():      # if any of the letters is lower case\n",
    "            upper_seq += 'C'    # all lower are converted to cysteins\n",
    "            print(infile1)      # to see how which files I got in my set\n",
    "    return header, upper_seq\n",
    "\n",
    "def write_upper(infile1):\n",
    "    '''\n",
    "    Calls find_lowerletters and writes what it returns \n",
    "    the same file truncating it.\n",
    "    '''\n",
    "    header, upper_seq = find_lowerletters(infile1)\n",
    "    with open(infile1, 'w') as wfile:\n",
    "        wfile.write(header+upper_seq)          #need to make new files or over write\n",
    "        wfile.truncate()\n",
    "    return \n",
    "\n",
    "def ids_list(infile2):                                              # call list of file names and for dsspfile\n",
    "    ''' Reads all lines from a file and saves them to a list. '''\n",
    "    cleanlines_list = []\n",
    "    with open(infile2) as ofile:\n",
    "        flist = ofile.readlines()# list of each line of the file excluding line 0 \n",
    "        for line in flist:\n",
    "            nonewline = line.rstrip()\n",
    "            cleanlines_list.append(nonewline)\n",
    "        return cleanlines_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ids_path = sys.argv[1]\n",
    "    ids = ids_list(ids_path)        # \"./list_of_blindset_ids_only\"\n",
    "    blind_set_path = sys.argv[2]\n",
    "    for ID in ids:\n",
    "        write_upper(os.path.join(blind_set_path, ID+\".fasta\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reran!\n",
    "--> No lowercase letters in my fasta blindset\n",
    "Updated script and fixed all files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking % of E, C, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first I want to read line 2 of each file \n",
    "# and save the string\n",
    "\n",
    "def count_C_H_E(infile1):                                              # call list of file names and for dsspfile\n",
    "    ''' Reads line 2 (index 1) from a fastalike dssp file. Returns a tuple \n",
    "    containing the number of C, H, E in that order. '''\n",
    "    cleanlines_string = ''\n",
    "    C=0\n",
    "    H=0\n",
    "    E=0\n",
    "    with open(infile1) as ofile:\n",
    "        flist = ofile.readlines()[1:] # list of each line of the file excluding line 0 \n",
    "        nonewline = flist[0].rstrip()\n",
    "        for character in nonewline:\n",
    "            if character == \"-\":\n",
    "                C+=1\n",
    "            elif character == 'H':\n",
    "                H+=1\n",
    "            elif character == 'E':\n",
    "                E+=1\n",
    "            else:\n",
    "                print(\"Unknown character!!!\", character)\n",
    "        return C, H, E    # returns number of C H and E\n",
    "    \n",
    "# a = lines_list(\"./blind_dssp/4ywn.dssp\")\n",
    "# print(a)\n",
    "\n",
    "def stitchingstrings(looplist):\n",
    "    all_list = []\n",
    "    sumC = 0           #defining final sums\n",
    "    sumH = 0\n",
    "    sumE = 0\n",
    "    totchar=0\n",
    "    percC=0\n",
    "    percH=0\n",
    "    percE=0\n",
    "    with open(looplist, 'r') as filenames:\n",
    "        all_list = filenames.readlines()  # rading file ids into list\n",
    "    for el in all_list:                   # for each filename\n",
    "        filename = el.rstrip()\n",
    "        C, H, E = count_C_H_E('trainingset/dssp/'+filename)         #  call function count_C_H_E with current filename\n",
    "        sumC += C                         # increment previous sum by each letter count\n",
    "        sumH += H\n",
    "        sumE += E\n",
    "    totchar += sumC + sumH + sumE\n",
    "    percC = sumC/totchar*100\n",
    "    percH = sumH/totchar*100\n",
    "    percE = sumE/totchar*100\n",
    "    return percC, percH, percE\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ila/01-Unibo/02_Lab2/files_lab2_project'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.91366317169069 37.68020969855832 24.40612712975098\n"
     ]
    }
   ],
   "source": [
    "C, H, E = stitchingstrings(\"dssp_filenames\")\n",
    "print(C, H, E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The blind_set contains\n",
    "\n",
    "37.91% C\n",
    "\n",
    "37.68% H\n",
    "\n",
    "24,41% E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.16215473786861 35.592731468128065 22.245113794003323\n"
     ]
    }
   ],
   "source": [
    "C, H, E = stitchingstrings(\"dssp_filenames_trainingset\")\n",
    "print(C, H, E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training_set contains\n",
    "\n",
    "42.16% C\n",
    "\n",
    "35.59% H\n",
    "\n",
    "22.24% E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Set | training | blind | \n",
    "|-----|----------|-------|\n",
    "| C   |42.16%|37.91%|\n",
    "| H   |35.59%|37.68%|\n",
    "| E   |22.24%|24,41%|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
