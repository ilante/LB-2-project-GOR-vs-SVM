{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the Blind Set\n",
    "\n",
    "* Also known as **holdout dataset**\n",
    "\n",
    "* Cross-validation is not sufficient tot estimate unbiased generalization performance.\n",
    "    - Model hyper-parameters are still optimized on the training set through cross-valiation and grid-search\n",
    "    - This may lead to some degree of overfitting on training data\n",
    "    - Using a blind set helps us to generate a 'never-seen-before condition'\n",
    "    \n",
    "## Generation Criteria:\n",
    "\n",
    "* Structures deposited **after** January 2015\n",
    "     - Release year of JPred4 is 2014\n",
    "* X-ray crystals with resolution < 2,5 $\\overset{\\circ}{A}$\n",
    "* Chain lenght in the range of 50 - 300 residues\n",
    "* Advanced search -> Entry Polymer Types:\n",
    "    - Protein OR Protein/NA \n",
    "* All pairs of sequences within the blind set should share less than 30% sequence identity ('internal redundancy'):\n",
    "- By using ```blastclust``` we can reduce the redundancy\n",
    "\n",
    "* When comparing sequences of the blindset with the JPRED set: All pairs (blind - Jpred) have an less than 30% identity  ('external redundancy')\n",
    "- This will be ensured using ```blastp```\n",
    "\n",
    "* The final blind set will comprise 150 proteins which will be randomly selected among those that meet the above criteria\n",
    "\n",
    "### 1. Downloading Data from the PDB\n",
    "\n",
    "Checked the boxes \"Entry ID\",\"Sequence\",\"Entity Polymer Type\",\"Chain ID\",\"Entry Id (Polymer Entity Identifiers)\".\n",
    "\n",
    "[here the link to my search](https://www.rcsb.org/search?request=%7B%22query%22%3A%7B%22type%22%3A%22group%22%2C%22logical_operator%22%3A%22and%22%2C%22nodes%22%3A%5B%7B%22type%22%3A%22group%22%2C%22logical_operator%22%3A%22and%22%2C%22nodes%22%3A%5B%7B%22type%22%3A%22group%22%2C%22logical_operator%22%3A%22and%22%2C%22nodes%22%3A%5B%7B%22type%22%3A%22terminal%22%2C%22service%22%3A%22text%22%2C%22parameters%22%3A%7B%22operator%22%3A%22greater%22%2C%22negation%22%3Afalse%2C%22value%22%3A%222015-01-31T00%3A00%3A00Z%22%2C%22attribute%22%3A%22rcsb_accession_info.deposit_date%22%7D%2C%22node_id%22%3A0%7D%2C%7B%22type%22%3A%22terminal%22%2C%22service%22%3A%22text%22%2C%22parameters%22%3A%7B%22operator%22%3A%22exact_match%22%2C%22negation%22%3Afalse%2C%22value%22%3A%22X-RAY%20DIFFRACTION%22%2C%22attribute%22%3A%22exptl.method%22%7D%2C%22node_id%22%3A1%7D%2C%7B%22type%22%3A%22terminal%22%2C%22service%22%3A%22text%22%2C%22parameters%22%3A%7B%22operator%22%3A%22less_or_equal%22%2C%22negation%22%3Afalse%2C%22value%22%3A2.5%2C%22attribute%22%3A%22rcsb_entry_info.resolution_combined%22%7D%2C%22node_id%22%3A2%7D%2C%7B%22type%22%3A%22group%22%2C%22logical_operator%22%3A%22or%22%2C%22nodes%22%3A%5B%7B%22type%22%3A%22terminal%22%2C%22service%22%3A%22text%22%2C%22parameters%22%3A%7B%22operator%22%3A%22exact_match%22%2C%22negation%22%3Afalse%2C%22value%22%3A%22Protein%20(only)%22%2C%22attribute%22%3A%22rcsb_entry_info.selected_polymer_entity_types%22%7D%2C%22node_id%22%3A3%7D%2C%7B%22type%22%3A%22terminal%22%2C%22service%22%3A%22text%22%2C%22parameters%22%3A%7B%22operator%22%3A%22exact_match%22%2C%22negation%22%3Afalse%2C%22value%22%3A%22Protein%2FNA%22%2C%22attribute%22%3A%22rcsb_entry_info.selected_polymer_entity_types%22%7D%2C%22node_id%22%3A4%7D%5D%7D%2C%7B%22type%22%3A%22terminal%22%2C%22service%22%3A%22text%22%2C%22parameters%22%3A%7B%22operator%22%3A%22range_closed%22%2C%22negation%22%3Afalse%2C%22value%22%3A%5B50%2C300%5D%2C%22attribute%22%3A%22entity_poly.rcsb_sample_sequence_length%22%7D%2C%22node_id%22%3A5%7D%5D%7D%5D%2C%22label%22%3A%22text%22%7D%5D%2C%22label%22%3A%22query-builder%22%7D%2C%22return_type%22%3A%22entry%22%2C%22request_options%22%3A%7B%22pager%22%3A%7B%22start%22%3A0%2C%22rows%22%3A100%7D%2C%22scoring_strategy%22%3A%22combined%22%2C%22sort%22%3A%5B%7B%22sort_by%22%3A%22score%22%2C%22direction%22%3A%22desc%22%7D%5D%7D%2C%22request_info%22%3A%7B%22src%22%3A%22ui%22%2C%22query_id%22%3A%22dc19df09287d4c5a80018000a03e2a6d%22%7D%7D)\n",
    "\n",
    "* Downloaded as CSV \n",
    "\n",
    "For some reason it automatically adds \"Entry ID\" as column 1. Whenever there is another chain of the same ID the first line of col 1 will be empty --> removed it using awk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head -6 1.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in {1..5}\n",
    "do\n",
    "    cat ${i}.csv | awk '{sub(/[^,]*/,\"\");sub(/,/,\"\")} 1' > ${i}new.csv \n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head -5 4new.csv #now it looks like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parsing and Filtering\n",
    "\n",
    "First I need to be aware that some of the chains are nucleic acids (\"NA-hybrid\").\n",
    "\n",
    "Remove all lines containing\n",
    "*  \"NA-hybrid\" \n",
    "*  \"DNA\"\n",
    "*  \"RNA\"\n",
    "\n",
    "to obtain protein sequences only!\n",
    "\n",
    "```  sed -n '/Protein/p' ./filename \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all lines that do NOT contain \"Protein\" --> this removes the header too!\n",
    "head -1 1new.csv > aa_only.csv # Adding correct header to top of file that will be appended all \"Protein\" lines\n",
    "\n",
    "for i in {1..5} # Appending only lines containing word \"Protein\"\n",
    "do\n",
    "    sed -n '/Protein/p' ${i}new.csv >> aa_only.csv \n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head aa_only.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grep \"Protein\" aa_only.csv | wc -l #  29638 Protein chains in the set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switched to python 3 kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading relevant packages:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "# sns.set() #do we really need that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aa = pd.read_csv(\"aa_only.csv\")\n",
    "df_aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where did the unnamed come from??\n",
    "unnamed = df_aa[\"Unnamed: 4\"] # Maybe trailing comma?\n",
    "unnamed.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding unique values in cloumn \"Entity Polymer Type\"\n",
    "I want to find unique values as described [here](https://chrisalbon.com/python/data_wrangling/pandas_list_unique_values_in_column/)\n",
    "\n",
    "This way I can be sure that my cleaning was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding unique names in cols\n",
    "pol_type = df_aa['Entity Polymer Type']\n",
    "pol_type.unique() # I am now sure that all DNA and RNA and Protein/NA lines have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -3 aa_only.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed '1d' aa_only.csv > noheader_aa_only.csv # removing header before generating fasta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l aa_only.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l noheader_aa_only.csv # value matches file is ok proceeding to make fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I want to consider the comma as a field sepparator \n",
    "\n",
    "Since there are several chains per entry denoted as e.g.:\n",
    "\n",
    "``` \"CAGTTTCAAACTC\",\"D, I\",\"5FD3\",```\n",
    "\n",
    "I need to remove the comma between the chains first and replace it with a space:\n",
    "*  ``` sed 's/\\, / /g' $i.csv ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing commas between chains\n",
    "\n",
    "sed 's/\\, //g' aa_only.csv > nospace.csv\n",
    "\n",
    "head nospace.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generating the FASTA file\n",
    "\n",
    "* Using awk: defining the comma as field sepparator.\n",
    "\n",
    "* ``` awk -F ',' ```\n",
    "\n",
    "* filtering for lenght in the range of 50 - 300 \n",
    "\n",
    "*  ``` 'length($1) > 50 && length($1) < 301 {print \">\"$4\":\"$3\"\\n\"$1}' ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat nospace.csv | awk -F ',' 'length($1) > 50 && length($1) < 301 {print \">\"$4\":\"$3\"\\n\"$1}' | sed 's/\\\"//g' > 50_300.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head 50_300.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequences containing X need to be removed:\n",
    "\n",
    "Working on the script and testing it along the way. The final result is saved as\n",
    "\n",
    "#### removeX.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lines_to_list(filename):\n",
    "    ''' Reads all lines from a file and saves them to a list. '''\n",
    "    content_list = []\n",
    "    with open(filename, \"r\") as rfile:\n",
    "        content_list = rfile.readlines()\n",
    "        return content_list\n",
    "\n",
    "myfastalist = lines_to_list(\"50_300.fasta\")  #works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(myfastalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(liste):\n",
    "    ''' Splits a evennumbered list into two lists. id_list contains all odd items while seq_list contains all even items. Returns the two lists.'''\n",
    "    id_list = liste[::2]\n",
    "    seq_list = liste[1::2]\n",
    "    return id_list, seq_list\n",
    "\n",
    "# teste = ['a', 'b', 'c', 'd', 'e', 'f'] #Works\n",
    "ids, seq = split_list(myfastalist)\n",
    "\n",
    "\n",
    "print(len(ids))\n",
    "print(len(seq))\n",
    "print(len(myfastalist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_X(id1, seq2):\n",
    "    '''Removes items containing X in the sequence list but also the ID in the ID list. \n",
    "    Returns an ID list and an'''\n",
    "    noXid = []\n",
    "    noXseq = []\n",
    "    for i in range(len(id1)):\n",
    "        flag = \"X\" in seq2[i]\n",
    "        if flag == False:\n",
    "            noXid.append(id1[i])\n",
    "            noXseq.append(seq2[i])\n",
    "    return noXid, noXseq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_id, new_seq = remove_X(ids, seq)\n",
    "\n",
    "print(len(new_id))\n",
    "print(len(new_seq))\n",
    "\n",
    "def no_X_id_and_seq(id_list, seq_list):\n",
    "    ''' Joins the lists to a big list containing both id and sequences. Returns one big list'''\n",
    "    biglist = []\n",
    "    for i in range(len(id_list)):\n",
    "        biglist.append(id_list[i])\n",
    "        biglist.append(seq_list[i])\n",
    "    return biglist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biglist = no_X_id_and_seq(new_id, new_seq)\n",
    "len(biglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing list to file:\n",
    "\n",
    "def list_to_fasta(liste):\n",
    "    '''Takes one id list and one sequlist as input. Writes all elements i to \n",
    "    a file. Returns the file.'''\n",
    "    with open('no_X.fasta', 'w') as F:\n",
    "        for i in liste:\n",
    "            F.write(str(i))\n",
    "    F.close            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_fasta(biglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short(infile, outfile):\n",
    "    del_seq_index = []\n",
    "    lines_list = []\n",
    "    with open(infile) as rfile:\n",
    "        lines_list = rfile.readlines()\n",
    "        for i in range(1, len(lines_list),2):\n",
    "            if len(lines_list[i]) < 7:\n",
    "                del_seq_index.append(i-1) # appending header index\n",
    "                del_seq_index.append(i)   # appending sequence index\n",
    "    with open(outfile, 'w') as wfile:\n",
    "        for i in range(len(lines_list)):\n",
    "            if i in del_seq_index:\n",
    "                continue\n",
    "            wfile.write(lines_list[i])    \n",
    "        \n",
    "filter_short(\"no_X.fasta\", 'longsequences.fasta')                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Clustering the Sequences in blastclust\n",
    "\n",
    "Sending file to be clustered to the VM:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scp no_X_long_sequs.fasta @proj:~/lb2-2020-project-englander\n",
    "\n",
    "screen\n",
    "\n",
    "source /opt/conda/bin/activate\n",
    "\n",
    "blastclust -i no_X_long_sequs.fasta -o final_clusters -S 30 -L 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Picking longest seuqence of each cluster: by default col1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cat final_clusters | awk $1 {print} > best_of_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in {1..5}\n",
    "do\n",
    "    mv $i.csv ./orignial_csv/$i.csv\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in {1..5}\n",
    "do\n",
    "     mv ${i}new.csv ./orignial_csv/${i}new.csv\n",
    "done     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Generating FASTA Containing ONLY Sequences of Best Cluster\n",
    "\n",
    "* I think the easiest is if I generate a list of best_of_final_cluster\n",
    "\n",
    "    - Need to first generate a new file that contains the \">\" character in front of every sequence\n",
    "\n",
    "\n",
    "* And generate a dictionary of the no_X_long_sequs.fasta   \n",
    "\n",
    "* Then I want to use the list to loop on the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure the output of the script will be fasta standard\n",
    "cat best_of_final_cluster | sed 's/^/>/' > crocodile_ids_final_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moved file in october:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/Users/ila/01-Unibo/02_Lab2/files_lab2_project/ztests_and_other_snippets/crocodile_ids_final_cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "def lines_to_list(infile1): \n",
    "    ''' Reads all lines from a file and saves them to a list. '''\n",
    "    content_list = []\n",
    "    with open(infile1, \"r\") as rfile:\n",
    "        content_list = rfile.readlines()\n",
    "        return content_list\n",
    "\n",
    "def split_list(infile1):\n",
    "    ''' Splits a evennumbered list into two lists. id_list contains \n",
    "    all odd items while seq_list contains all even items. Returns the two lists.'''\n",
    "    myfastalist = lines_to_list(infile1)  #works\n",
    "    id_list = myfastalist[::2]\n",
    "    seq_list = myfastalist[1::2]\n",
    "    return id_list, seq_list\n",
    "\n",
    "def dict_from_lists(infile1):\n",
    "    '''Takes feeds two lists into a dictionary. \n",
    "    Returns the dicitonary'''\n",
    "    id_list, seq_list = split_list(infile1)\n",
    "    keys = id_list\n",
    "    values = seq_list\n",
    "    full_dict = dict(zip(keys, values))\n",
    "    return full_dict\n",
    "    \n",
    "def keep_whats_in_dict(infile1, infile2, outfile):\n",
    "    '''Loops through a list and a dictionary. Appending the values\n",
    "    of the list (PDB ids which are also the keys of the dictionary) and the\n",
    "    values of the dictionary to the outfile.'''\n",
    "    idlist = lines_to_list(infile2) # reading ids from file into list\n",
    "    aa_dict = dict_from_lists(infile1)\n",
    "    with open(outfile, 'a') as afile:\n",
    "        for i in idlist:\n",
    "            afile.write(i) #appending ID in even lines\n",
    "            afile.write(aa_dict[i]) # appending value (sequ) in odd lines\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    infile1 = sys.argv[1]\n",
    "    infile2 = sys.argv[2]\n",
    "    outfile = sys.argv[3]\n",
    "    keep_whats_in_dict(infile1, infile2, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the script\n",
    "python  make_fasta_from_best_of_each_cluster.py no_X_long_sequs.fasta crocodile_ids_final_cluster crocodile_best_of_final_cluster.fasta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File location changed in October\n",
    "\n",
    "/Users/ila/01-Unibo/02_Lab2/files_lab2_project/ztests_and_other_snippets/crocodile_best_of_final_cluster.fasta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scp -i ~/.ssh/id_rsa.pub ./make_fasta_from_best_of_each_cluster.py proj:~/lb2-2020-project-englander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scp -i ~/.ssh/id_rsa.pub ./crocodile_best_of_final_cluster.fasta proj:~/lb2-2020-project-englander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Mergeing all fasta files of the jpred (training) set \n",
    "\n",
    "Need it later to generate blastdb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat *.fasta > JPREDall.fasta  # merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grep \">\" JPREDall.fasta | wc -l  # works --> merged all 1348 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scp -i ~/.ssh/id_rsa.pub ./JPREDall.fasta proj:~/lb2-2020-project-englander"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "makeblastdb -in ./makeblastdb/JPREDall.fasta -dbtype prot\n",
    "\n",
    "Building a new DB, current time: 09/23/2020 14:29:12\n",
    "New DB name:   /home/um19/lb2-2020-project-englander/makeblastdb/JPREDall.fasta\n",
    "New DB title:  ./makeblastdb/JPREDall.fasta\n",
    "Sequence type: Protein\n",
    "Keep MBits: T\n",
    "Maximum file size: 1000000000B\n",
    "Adding sequences from FASTA; added 1348 sequences in 0.06898 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Reducing Redundancy\n",
    "\n",
    "I want to produce a blind testset that is as dissimilar to the training set as possible.\n",
    "\n",
    "* Running blastp with blindset against JPRED training set\n",
    "* I dentifying all sequences in the blindset that have LESS than 30% seq ID with any other sequ in the training set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " blastp -query ../crocodile_best_of_final_cluster.fasta -db JPREDall.fasta -evalue 0.01 -out hits.blastp.tab -outfmt 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying hits.blast.tab to local\n",
    "scp -i ~/.ssh/um19_id_rsa um19@m19.lsb.biocomp.unibo.it:~/lb2-2020-project-englander/makeblastdb/hits.blastp.tab ~/01-Unibo/02_Lab2/project_blindset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     759 ../ztests_and_other_snippets/hits.blastp.tab\n"
     ]
    }
   ],
   "source": [
    "wc -l ../ztests_and_other_snippets/hits.blastp.tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5MA4:A\td4duia_\t83.439\t157\t26\t0\t3\t159\t7\t163\t1.87e-86\t251\n",
      "5MA4:A\td4duia_\t77.444\t133\t29\t1\t165\t296\t31\t163\t5.71e-63\t191\n",
      "6D8X:A\td1fcya_\t29.032\t186\t126\t3\t109\t292\t53\t234\t1.41e-19\t80.5\n",
      "6G4T:A\td3k34a_\t56.757\t259\t111\t1\t7\t265\t1\t258\t2.30e-110\t314\n"
     ]
    }
   ],
   "source": [
    "head -4 ../ztests_and_other_snippets/hits.blastp.tab\n",
    "# $1 pdb_Id $2 jprd id $3 % identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Non-Redundant Set With Least Similarty \n",
    "\n",
    "Step 3 from the Slides: \"Filter out from the preliminary chain set, all chains having at least one BLAST hit with SI >= 30% with any sequence in the JPRED4 dataset\n",
    "\n",
    "### Generating 2 Files Containing Only Relevant Lines\n",
    "\n",
    "file above30:\n",
    "* I'll extract ```$1``` if ``` $3 > 30 ```\n",
    "\n",
    "file below30\n",
    "* I have to extract ```$1``` if col ```$3 < 30 ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# awk keep filed if col 3 < 30 pipe to new file.\n",
    "awk -F ' ' '$3 < 30 {print $1 \" \" $3}' hits.blastp.tab > below_30.hits\n",
    "awk -F ' ' '$3 >= 30 {print $1 \" \" $3}' hits.blastp.tab > above_30.hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keeping IDs only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awk -F ' ' '$3 < 30 {print $1}' hits.blastp.tab > id_below_30.hits\n",
    "awk -F ' ' '$3 >= 30 {print $1}' hits.blastp.tab > id_above_30.hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lines_to_list(infile1): \n",
    "    ''' Reads all lines from a file and saves them to a list. '''\n",
    "    content_list = []\n",
    "    with open(infile1, \"r\") as rfile:\n",
    "        content_list = rfile.readlines()\n",
    "        return content_list\n",
    "        \n",
    "below = lines_to_list(\"id_below_30.hits\")  # list of all ids scoring below 30% id\n",
    "above = lines_to_list('id_above_30.hits')  # list of all ids scoring above and equal to 30% id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(below))\n",
    "print(len(above))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"6D8X:A\\n\" in above\n",
    "# head -30 id_below_30.hits > ids_test\n",
    "# head -30 id_above_30.hits > ids_above_test\n",
    "\n",
    "def remove_matches(lower, higher):\n",
    "    '''Takes two lists as input and returns a list that contains\n",
    "    all values of 'lower' values that are NOT element of 'higher'.'''\n",
    "    keepers = []          # list holding all ids that have not scored >= 30% with any of the JPRED sequnces\n",
    "    for i in lower:       \n",
    "        if i not in higher: \n",
    "            keepers.append(i)  # keeps only ids that are not reported in the list \"above\"\n",
    "    return keepers\n",
    "\n",
    "keep = remove_matches(below, above) \n",
    "\n",
    "print(len(keep))\n",
    "# Have 177 unique sequnces with no match above 30% id with any other sequ in the testing (JPRED) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = lines_to_list(\"best_of_final_cluster\") # generating list of all ids that were in the blastp input\n",
    "print(len(all_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_mis_matches(biglist, partiallist):\n",
    "    '''Stores exclusively biglist values that are not reported in partiallist.\n",
    "    Returns the biglist with all partiallist matches removed. Keeps all values \n",
    "    that donot match any element of partiallist in a new list. Returns the new list.'''\n",
    "    keepers = []\n",
    "    for i in biglist:\n",
    "        if i not in partiallist:\n",
    "            keepers.append(i) \n",
    "    return keepers\n",
    "\n",
    "all_without_above = keep_mis_matches(all_ids, above)\n",
    "len(all_without_above)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_to_file(liste, newfile):\n",
    "    '''Takes as input a list and writes each element to a new file'''\n",
    "    with open(newfile, 'a') as afile: \n",
    "        for i in liste:\n",
    "            afile.write(i)\n",
    "            \n",
    "write_list_to_file(all_without_above, 'try_again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_scoring_below30.py\n",
    "\n",
    "# input of blastp = fastafile find missing\n",
    "\n",
    "# interseciton of set below() and above() --> throw_away\n",
    "# find and remove throw away id file\n",
    "\n",
    "# make list of all ids reproted in fastinputblastp\n",
    "# turn fastinputblastp_list into fastinputblastp_set (biggest set)\n",
    "# fastinputblastp_set - above_set \n",
    "\n",
    "def lines_to_list(infile1): \n",
    "    ''' Reads all lines from a file and saves them to a list. '''\n",
    "    content_list = []\n",
    "    with open(infile1, \"r\") as rfile:\n",
    "        content_list = rfile.readlines()\n",
    "        return content_list\n",
    "        \n",
    "below = lines_to_list(\"id_below_30.hits\")  # list of all ids scoring below 30% id\n",
    "above = lines_to_list('id_above_30.hits')  # list of all ids scoring above and equal to 30% id\n",
    "\n",
    "def remove_matches(lower, higher):\n",
    "    '''Takes two lists as input and returns a list that contains\n",
    "    all values of 'lower' values that are NOT element of 'higher'.'''\n",
    "    keepers = []          # list holding all ids that have not scored >= 30% with any of the JPRED sequnces\n",
    "    for i in lower:       \n",
    "        if i not in higher: \n",
    "            keepers.append(i)  # keeps only ids that are not reported in the list \"above\"\n",
    "    return keepers\n",
    "\n",
    "keep = remove_matches(below, above) \n",
    "\n",
    "# print(len(keep))\n",
    "# Have 177 unique sequnces with no match above 30% id with any other sequ in the testing (JPRED) set.\n",
    "\n",
    "##########################################################\n",
    "# Making a list of all ids (input IDs of the blastp)\n",
    "##########################################################\n",
    "\n",
    "all_ids = lines_to_list(\"best_of_final_cluster\") # generating list of all ids that were in the blastp input\n",
    "# print(len(all_ids))\n",
    "\n",
    "def keep_mis_matches(biglist, partiallist):\n",
    "    '''Stores exclusively biglist values that are not reported in partiallist.\n",
    "    Returns the biglist with all partiallist matches removed. Keeps all values \n",
    "    that donot match any element of partiallist in a new list. Returns the new list.'''\n",
    "    keepers = []\n",
    "    for i in biglist:\n",
    "        if i not in partiallist:\n",
    "            keepers.append(i) \n",
    "    return keepers\n",
    "\n",
    "all_without_above = keep_mis_matches(all_ids, above)\n",
    "# len(all_without_above)\n",
    "\n",
    "def write_list_to_file(liste, newfile):\n",
    "    '''Takes as input a list and writes each element to a new file'''\n",
    "    with open(newfile, 'a') as afile: \n",
    "        for i in liste:\n",
    "            afile.write(i)\n",
    "            \n",
    "write_list_to_file(all_without_above, 'try_again')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly Sort and Pick 150 Sequences \n",
    "\n",
    "Sometimes not all IDS have an associated PDB file thus I select 160 to be sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ids_0-30 | sort -R | head -160 > random.blindset2 # in case some PDB files are not available\n",
    "!wc -l random.blindset2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I still have *NOT* removed identical chain IDs:\n",
    "\n",
    "* The file still contains all letters denoting different chains after the \":\"\n",
    "\n",
    "* printing it to a new file only keeping first letter after the \":\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail random.blindset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat random.blindset2 | cut -c-6 > id_and_chain_blindset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if really no above 30 are in the new list\n",
    "def lines_to_list(infile1): \n",
    "    ''' Reads all lines from a file and saves them to a list. '''\n",
    "    content_list = []\n",
    "    with open(infile1, \"r\") as rfile:\n",
    "        content_list = rfile.readlines()\n",
    "        return content_list\n",
    "        \n",
    "bigset = lines_to_list(\"random.blindset2\")  # list of all ids scoring below 30% id\n",
    "above = lines_to_list('id_above_30.hits')  # list of all ids scoring above and equal to 30% id\n",
    "\n",
    "def remove_matches(l1, l2):\n",
    "    '''Takes two lists as input and returns a list that contains\n",
    "    all values of 'lower' values that are NOT element of 'higher'.'''\n",
    "    keepers = []          # list holding all ids that have not scored >= 30% with any of the JPRED sequnces\n",
    "    for i in l1:       \n",
    "        if i not in l2: \n",
    "            keepers.append(i)  # keeps only ids that are not reported in the list \"above\"\n",
    "    return keepers\n",
    "\n",
    "keep = remove_matches(bigset, above) \n",
    "len(keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Download Structures from PDB\n",
    "\n",
    "For each of the 150 sequences I have to download the pdb structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat random.blindset2 | cut -c-4 > only_id_blindset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in\n",
    "# https://files.rcsb.org/view/6OZJ.pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Generate DSSP files from selected PDB files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lines_to_list(infile1): \n",
    "    ''' Reads all lines from a file and saves them to a list. '''\n",
    "    content_list = []\n",
    "    with open(infile1, \"r\") as rfile:\n",
    "        content_list = rfile.readlines()\n",
    "        return content_list\n",
    "        \n",
    "bigset = lines_to_list(\"only_id_blindset2\") # Contain trailing newline char\n",
    "#removing \\n :\n",
    "def rstrip_each_item(list):\n",
    "    my_150_PDB = []\n",
    "    for i in list:\n",
    "        clean = i.rstrip()\n",
    "        my_150_PDB.append(clean)\n",
    "    return my_150_PDB\n",
    "\n",
    "my_150_PDB = rstrip_each_item(bigset)\n",
    "print(my_150_PDB)\n",
    "len(my_150_PDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat only_id_blindset2 | sed -z 's/\\n/, /g' > commas2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd blindset_all_PDBs/150_blind_PDBs/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in {1..150}\n",
    "do\n",
    "    gunzip *ent.gz\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Generating DSSP files from all 150 PDB files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "# Running the DSSP on the extracted PDB files\n",
    "for i in *.ent\n",
    "do\n",
    "        mkdssp -i \"$i\" -o \"./dsspout/$i.dssp\"\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " scp -i ~/.ssh/id_rsa.pub ./150_blind_PDBs/* proj:~/lb2-2020-project-englander/150_blind_PDBs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Desired Chain From Each DSSP file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting chain and secondary structure from DSSP files:\n",
    "\n",
    "* protein chain: ```$3```\n",
    "* Secondary Structure Summanry ```$4```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls id_and_chain_blindset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /Users/ila/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ila/01-Unibo/02_Lab2/project_blindset'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewinding Undoing Some Mess\n",
    "Forgot that I downloaded 160 structures --> since not all were available in PDB format I just took a little extra. Then I only unzipped 150. But which did I NOT use? \n",
    "Who knows - sloppy documentation -_-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4Y4O,4YBB,4Y4O,5XJL,6EHA,6NTV,6T8S,4YWN,5FLY,5JVV,"
     ]
    }
   ],
   "source": [
    "def lines_list(fname):\n",
    "    with open(fname) as ofile:\n",
    "        flist = ofile.readlines() # returns list containing each line of the file\n",
    "        return flist\n",
    "\n",
    "dsspinfo = lines_list(\"/Users/ila/01-Unibo/02_Lab2/project_blindset/deletechainmess/dssp_file_names\")  # generating list of all file names e.g. \"pdb7jtl.ent.dssp\"\n",
    "\n",
    "def caps_dssp_list(li): \n",
    "    '''Takes as input raw DSSP list and returns the pdb IDs of each file in a list.'''\n",
    "    dssp_id_list =[]\n",
    "    for i in range(len(li)):  \n",
    "        el = dsspinfo[i]             # isolating list el\n",
    "        new_el = el[3:7].upper()     # creating new_el wich is only the fileds containing the PDB id\n",
    "        dssp_id_list.append(new_el+\"\\n\")  # append each id to dssp_list\n",
    "    return dssp_id_list\n",
    "\n",
    "dssp_ids = caps_dssp_list(dsspinfo)\n",
    "\n",
    "pdb_ids_160 =lines_list(\"only_id_blindset2\")\n",
    "\n",
    "# print(pdb_ids_160)\n",
    "\n",
    "def keep_matches(biglist, partiallist):\n",
    "    '''Stores all common values that are reported in both lists.\n",
    "    Returns the intersection of items as keepers list.'''\n",
    "    keepers = []\n",
    "    for i in biglist:\n",
    "        if i in partiallist:\n",
    "            keepers.append(i) \n",
    "    return keepers\n",
    "\n",
    "all150dssp = keep_matches(pdb_ids_160, dssp_ids)\n",
    "# print(all150dssp)\n",
    "\n",
    "def remove_matches(l1, l2):\n",
    "    '''Takes two lists as input and returns a list that contains\n",
    "    all values of 'lower' values that are NOT element of 'higher'.'''\n",
    "    keepers = []          # list holding all ids that have not scored >= 30% with any of the JPRED sequnces\n",
    "    for i in l1:       \n",
    "        if i not in l2: \n",
    "            keepers.append(i)  # keeps only ids that are not reported in the list \"above\"\n",
    "    return keepers\n",
    "\n",
    "keep = remove_matches(pdb_ids_160, dssp_ids) \n",
    "\n",
    "for i in keep:\n",
    "    print(i.rstrip()+\",\", end=\"\") # deleted them manually  -_- brainfog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unused:\n",
    "From the origninal download I did NOT use the following PDB ids:\n",
    "   * 4Y4O,4YBB,4Y4O,5XJL,6EHA,6NTV,6T8S,4YWN,5FLY,5JVV,\n",
    "   * saved them to ```~/lb2-2020-project-englander/unused_10```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdb4ywn.ent                                   100%  395KB 930.2KB/s   00:00    \n",
      "pdb5xjl.ent                                   100%  441KB 800.6KB/s   00:00    \n",
      "pdb5fly.ent                                   100% 1257KB   1.2MB/s   00:01    \n",
      "pdb5jvv.ent                                   100%  880KB 947.6KB/s   00:00    \n",
      "pdb6ntv.ent                                   100%  481KB   1.0MB/s   00:00    \n",
      "pdb6eha.ent                                   100%  374KB   1.2MB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "#sending them to the VM for safekeeping (incase I need to replace any of the files generated)\n",
    "# the -r argument --> recursively copy these files to the VM\n",
    "scp -i ~/.ssh/id_rsa.pub -r ~/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/unused_10/ proj:~/lb2-2020-project-englander/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdb4ywn.ent.dssp                              100%   41KB 414.3KB/s   00:00    \n",
      "pdb6eha.ent.dssp                              100%   61KB 565.7KB/s   00:00    \n",
      "pdb6ntv.ent.dssp                              100%   95KB 411.9KB/s   00:00    \n",
      "pdb5fly.ent.dssp                              100%   78KB 565.8KB/s   00:00    \n",
      "pdb5xjl.ent.dssp                              100%   86KB 913.9KB/s   00:00    \n",
      "pdb5jvv.ent.dssp                              100%   83KB 563.4KB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "# ran dssp sepperately on these files to have some backup inscase some dont have what they need\n",
    "scp -i ~/.ssh/um19_id_rsa -r um19@m19.lsb.biocomp.unibo.it:~/lb2-2020-project-englander/unused_10/dsspout_unused10/ ~/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/unused_10/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The function\n",
    "\n",
    "# extract_ss_from_dssp_using_2input_files.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lines_list(infile1):                                              # call list of file names and for dsspfile\n",
    "    ''' Reads all lines from a file and saves them to a list. '''\n",
    "    with open(infile1) as ofile:\n",
    "        flist = ofile.readlines() # returns list containing each line of the file\n",
    "        return flist\n",
    "\n",
    "def relevant_lines(infile1, desired_chain):\n",
    "    '''Takes list (extracted from a DSSP file) and the name of the desired_chain as input.\n",
    "    Returns 2 strings: ss_string holds the secondary structure mapping and aa_string holds \n",
    "    the amino acid information. Missing residues (when no atomic information of the PDB is \n",
    "    present) are assigned the letter \"C\" (coil) in the ss_string and \"X\" in the aa_string.'''\n",
    "    dssp_list = lines_list(infile1)     # contains all lines from the dssp file.\n",
    "    relevant = False # boolean variable            \n",
    "#     desired_chain = \"A\"                            # change to load from \"id_and_chain_blindset2\"\n",
    "    ss_string = ''\n",
    "    aa_string = ''\n",
    "    for line in dssp_list:\n",
    "        if '#' in line: # find last line before relevant output\n",
    "            relevant =True   # flips rel to true - so the folowing lines are saved\n",
    "            continue\n",
    "        if relevant:\n",
    "            if line[11] == desired_chain:\n",
    "                ss_string += line[16]\n",
    "                if line[13] == \"!\":\n",
    "                    aa_string += \"X\"\n",
    "                else:\n",
    "                    aa_string += line[13]\n",
    "    return ss_string, aa_string\n",
    "\n",
    "def raw_to_threclasses(rawstring):\n",
    "        structure_dict = {\"H\":\"H\", \"G\":\"H\", \"I\":\"H\", \"B\":\"E\", \"E\":\"E\", \"T\":\"C\", \"S\":\"C\", \" \":\"C\"} \n",
    "        threeclasses = ''\n",
    "        for letter in rawstring:\n",
    "                threeclasses += structure_dict[letter]\n",
    "        return threeclasses    \n",
    "    \n",
    "def generate_dssp_fasta(filename_id, chain): \n",
    "    '''Writes SS to dsspfile and AA to fastafile'''\n",
    "    # reads dssp file and returns ss_string and aa_string.\n",
    "    ss_string, aa_string = relevant_lines(\"/Users/ila/01-Unibo/02_Lab2/project_blindset/deletechainmess/testruns/pdb\"+filename_id+\".ent.dssp\", chain)     \n",
    "    with open(filename_id+\".dssp\", 'w') as dsspfile:\n",
    "        ss = raw_to_threclasses(ss_string)\n",
    "        dsspfile.write(\">\"+filename_id+\"_\"+chain+\"\\n\")\n",
    "        dsspfile.write(ss+\"\\n\")\n",
    "        \n",
    "    with open(filename_id+\".fasta\", 'w') as fastafile:    \n",
    "        fastafile.write(\">\"+filename_id+\"_\"+chain+\"\\n\")\n",
    "        fastafile.write(aa_string+\"\\n\")\n",
    "        if aa_string == \"\":\n",
    "                print('empty',filename_id, chain, end=\"\")\n",
    "        \n",
    "# creating list holding all pdb ids and chain descriptions        \n",
    "id_chain = lines_list('/Users/ila/01-Unibo/02_Lab2/project_blindset/deletechainmess/BLINDset_id_and_chain')  \n",
    "\n",
    "for el in id_chain:            # each el is like \"6LTZ:A\"\n",
    "    field_list = el.split(':') # list  BOTH contains ID [0] and chain [1]\n",
    "    fname_id =  field_list[0].lower()                   #\n",
    "#     fname = \"pdb\"+field_list[0].lower()+\".ent.dssp\"     #fname to be used in the function calls\n",
    "    chain = field_list[1].rstrip()                             # chain name to be used in each function call\n",
    "    generate_dssp_fasta(fname_id, chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ila/01-Unibo/02_Lab2/files_lab2_project/documentation_notebooks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6LTZ:A\\n', '6EXX:A\\n', '5XGA:A\\n', '5WUJ:B\\n', '4ZY7:A\\n', '5F2A:A\\n', '5D1R:A\\n', '5UNI:A\\n', '5U7E:A\\n', '5ANP:A\\n', '6HKS:A\\n', '6J0Y:C\\n', '6L77:A\\n', '6KKO:A\\n', '6GW6:B\\n', '5LDD:B\\n', '6K7Q:A\\n', '4Y0L:A\\n', '5GNA:B\\n', '5C8A:A\\n', '4ZC4:A\\n', '6EI6:A\\n', '5UMV:A\\n', '6OR3:A\\n', '5U4U:A\\n', '5XKS:A\\n', '5GHL:A\\n', '5XYF:A\\n', '5N07:A\\n', '5FB9:A\\n', '5CEG:A\\n', '7BWF:B\\n', '4YTE:A\\n', '5V0M:A\\n', '6SE1:A\\n', '5UC0:A\\n', '5WNW:A\\n', '5IHF:A\\n', '5T2Y:A\\n', '5IB0:A\\n', '6USC:A\\n', '4UIQ:A\\n', '6YJ1:A\\n', '6DHX:A\\n', '5D6T:A\\n', '6DN4:A\\n', '5BP5:C\\n', '6ISU:A\\n', '6FSF:A\\n', '5VOG:A\\n', '5IR2:A\\n', '5D71:A\\n', '5BPX:A\\n', '5II0:A\\n', '4Y0O:A\\n', '5AUN:A\\n', '5C5Z:A\\n', '5KWV:A\\n', '6MDW:A\\n', '5FQ0:A\\n', '7BVV:A\\n', '5AV5:A\\n', '5FFL:A\\n', '6OOD:A\\n', '5KQA:A\\n', '5DCF:A\\n', '5GKE:A\\n', '5ZRY:A\\n', '5UIV:A\\n', '6GBI:A\\n', '5MC9:B\\n', '6FWT:A\\n', '6HSV:A\\n', '6HFG:B\\n', '5A88:A\\n', '5YEI:B\\n', '6NDR:A\\n', '5KLC:A\\n', '6MAB:A\\n', '6AOZ:A\\n', '5T2X:A\\n', '5LTF:A\\n', '6J19:B\\n', '6T7O:A\\n', '6MLX:A\\n', '5YMX:A\\n', '7JTL:A\\n', '5K21:A\\n', '5CTD:C\\n', '6R5W:A\\n', '5EIV:A\\n', '5HT8:A\\n', '5DD8:A\\n', '5D16:A\\n', '4ZEY:A\\n', '6VCI:A\\n', '5Y7W:A\\n', '4ZLR:A\\n', '5XVK:A\\n', '6VK4:D\\n', '6IA7:A\\n', '5HJF:A\\n', '6QLC:A\\n', '6WK3:A\\n', '5WOQ:A\\n', '5YSN:B\\n', '5Z1N:A\\n', '5BPU:A\\n', '5ABR:A\\n', '4ZKP:A\\n', '6ON1:A\\n', '6I9L:A\\n', '5AZW:A\\n', '6KOK:A\\n', '5Y4R:C\\n', '5CYB:A\\n', '5A6W:C\\n', '5U5N:A\\n', '5BN2:A\\n', '6UXF:A\\n', '6K6L:A\\n', '5WD6:A\\n', '6OVI:A\\n', '5V2I:A\\n', '6IQO:A\\n', '6MD3:A\\n', '5JSN:B\\n', '5JWO:A\\n', '6Q7N:A\\n', '5NL9:A\\n', '6Q8J:A\\n', '5WD8:A\\n', '6CEQ:A\\n', '5BPK:C\\n', '5BXQ:A\\n', '6OKM:R\\n', '5MMH:A\\n', '6H9E:A\\n', '5F1S:A\\n', '6SLK:B\\n', '5DG6:A\\n', '5DQ0:A\\n', '5X4B:A\\n', '5B71:E\\n', '4ZRZ:A\\n', '5M9O:A\\n', '6DEW:A\\n', '5U39:A\\n', '6R82:A\\n', '6G3Z:A\\n']\n"
     ]
    }
   ],
   "source": [
    "def lines_list(infile1):                                              # call list of file names and for dsspfile\n",
    "    ''' Reads all lines from a file and saves them to a list. '''\n",
    "    with open(infile1) as ofile:\n",
    "        flist = ofile.readlines() # returns list containing each line of the file\n",
    "        return flist\n",
    "    \n",
    "def findX(filename):\n",
    "    with open(filename+\".fasta\") as myfasta:\n",
    "        id = myfasta.readline()\n",
    "        sequ = myfasta.readline()\n",
    "        if 'X' in sequ == True:\n",
    "            print(filename, sequ)\n",
    "        else:\n",
    "            print(\"no X\")\n",
    "\n",
    "# id_chain = lines_list('/Users/ila/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/BLINDset_id_and_chain')   \n",
    "id_chain = lines_list('/Users/ila/01-Unibo/02_Lab2/files_lab2_project/all_data/blindset/blindset_all_PDBs/150_blind_PDBs/BLINDset_id_and_chain')   \n",
    "\n",
    "for el in id_chain:            # each el is like \"6LTZ:A\"\n",
    "    field_list = el.split(':') # list  BOTH contains ID [0] and chain [1]\n",
    "    fname_id =  field_list[0].lower()                   #\n",
    "#     fname = \"pdb\"+field_list[0].lower()+\".ent.dssp\"     #fname to be used in the function calls\n",
    "    chain = field_list[1].rstrip()                             # chain name to be used in each function call\n",
    "    findX(fname_id)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6j19.fasta less than 50\n"
     ]
    }
   ],
   "source": [
    "def lines_list(infile1):                                              # call list of file names and for dsspfile\n",
    "    ''' Reads all lines from a file and saves them to a list. '''\n",
    "    with open(infile1) as ofile:\n",
    "        flist = ofile.readlines() # returns list containing each line of the file\n",
    "        return flist\n",
    "    \n",
    "def count_len_seq(filename):\n",
    "    with open('/Users/ila/01-Unibo/02_Lab2/project_blindset/blind_fasta/'+filename) as myfasta:\n",
    "        id = myfasta.readline()\n",
    "        sequ = myfasta.readline()\n",
    "        lseq = len(sequ)\n",
    "        if lseq <= 50:\n",
    "            print(filename, \"less than 50\")\n",
    "        if lseq >= 300:\n",
    "            print(filename, 'more than 300')\n",
    "\n",
    "id_chain = lines_list('/Users/ila/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/BLINDset_id_and_chain')              \n",
    "\n",
    "for el in id_chain:            # each el is like \"6LTZ:A\"\n",
    "    field_list = el.split(':') # list  BOTH contains ID [0] and chain [1]\n",
    "    fname_id =  field_list[0].lower()\n",
    "#     fname = \"pdb\"+field_list[0].lower()+\".ent.dssp\"     #fname to be used in the function calls\n",
    "    chain = field_list[1].rstrip()                             # chain name to be used in each function call\n",
    "    count_len_seq(fname_id+'.fasta')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removed \n",
    "\n",
    "6J19:B\n",
    "    \n",
    "from \n",
    "\n",
    "BLINDset_id_and_chain    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing 6j19.fasta and 6j19.dssp with 4ywn\n",
    "\n",
    "6j19 has < 50 aa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing from \"unused set\" in\n",
    "``` ~/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/unused_10/unused_10_fasta```\n",
    "\n",
    "List of unused sequs and chains\n",
    "```['5XJL:2', '6EHA:A', '6NTV:A', '4YWN:A', '5FLY:A', '5FLY:A']```\n",
    "\n",
    "replacing 9j19.fasta and dssp with 4ywn_A -- to set of 150 sequs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ila/01-Unibo/02_Lab2/project_blindset\n"
     ]
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv /Users/ila/01-Unibo/02_Lab2/project_blindset/blind_dssp/6j19.dssp ~/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/unused_10/unused_9_dssp/ \n",
    "mv /Users/ila/01-Unibo/02_Lab2/project_blindset/blind_fasta/6j19.fasta ~/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/unused_10/unused_10_fasta/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4ywn.fasta\n",
      "5fly.fasta\n",
      "5xjl.fasta\n",
      "6eha.fasta\n",
      "6j19.fasta\n",
      "6ntv.fasta\n"
     ]
    }
   ],
   "source": [
    "ls -1 ~/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/unused_10/unused_10_fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv ~/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/unused_10/unused_9_dssp/4ywn.dssp  /Users/ila/01-Unibo/02_Lab2/files_lab2_project/all_data/blindset/blind_dssp/\n",
    "mv ~/01-Unibo/02_Lab2/project_blindset/blindset_all_PDBs/150_blind_PDBs/unused_10/unused_10_fasta/4ywn.fasta  /Users/ila/01-Unibo/02_Lab2/files_lab2_project/all_data/blindset/blind_fasta/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrected the mistake have now 150 dssp and fasta of sufficient lenght\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     150\n"
     ]
    }
   ],
   "source": [
    "ls  /Users/ila/01-Unibo/02_Lab2/files_lab2_project/all_data/blindset/blind_fasta/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     150\n"
     ]
    }
   ],
   "source": [
    "ls  /Users/ila/01-Unibo/02_Lab2/files_lab2_project/all_data/blindset/blind_dssp/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       1\n"
     ]
    }
   ],
   "source": [
    "cat /Users/ila/01-Unibo/02_Lab2/files_lab2_project/all_data/blindset/blind_fasta/*.fasta | grep -v '>' | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just checking if I have any lower case letters indicating C-C bridges in any of the fastafiles of my blindset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4uiq\n",
      "4y0l\n",
      "4y0o\n",
      "4yte\n",
      "4ywn\n",
      "4zc4\n",
      "4zey\n",
      "4zkp\n",
      "4zlr\n",
      "4zrz\n"
     ]
    }
   ],
   "source": [
    "!head list_of_blindset_ids_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``` project_blindset/scripts/lowercase_infile.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/anaconda3/bin/python\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# script to check if there are any lowercase letters corresponding to SS bridges (Cysteins).\n",
    "def find_lowerletters(infile1):\n",
    "    ''' \n",
    "    Reads lines from a file  EXCLUDING line 1. Saves string to a variable.\n",
    "    Checks each character. If character is lower case it is replaced by a C. As all dssp files designate\n",
    "    SS-bridges with lower case pairs.\n",
    "    '''\n",
    "    with open(infile1) as ofile:\n",
    "        lines = ofile.readlines()\n",
    "        header = lines[0]\n",
    "        sequence = lines[1] # list of each line of the file excluding line 0  \n",
    "        upper_seq = ''\n",
    "    # check_string = lines_list(infile1)[0] # l[0] to check_string\n",
    "    for char in sequence:\n",
    "        if char.isupper():\n",
    "            upper_seq += char\n",
    "        if char.islower():      # if any of the letters is lower case\n",
    "            upper_seq += 'C'    # all lower are converted to cysteins\n",
    "            print(infile1)      # to see how which files I got in my set\n",
    "    return header, upper_seq\n",
    "\n",
    "def write_upper(infile1):\n",
    "    '''\n",
    "    Calls find_lowerletters and writes what it returns \n",
    "    the same file truncating it.\n",
    "    '''\n",
    "    header, upper_seq = find_lowerletters(infile1)\n",
    "    with open(infile1, 'w') as wfile:\n",
    "        wfile.write(header+upper_seq)          #need to make new files or over write\n",
    "        wfile.truncate()\n",
    "    return \n",
    "\n",
    "def ids_list(infile2):                                              # call list of file names and for dsspfile\n",
    "    ''' Reads all lines from a file and saves them to a list. '''\n",
    "    cleanlines_list = []\n",
    "    with open(infile2) as ofile:\n",
    "        flist = ofile.readlines()# list of each line of the file excluding line 0 \n",
    "        for line in flist:\n",
    "            nonewline = line.rstrip()\n",
    "            cleanlines_list.append(nonewline)\n",
    "        return cleanlines_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ids_path = sys.argv[1]\n",
    "    ids = ids_list(ids_path)        # \"./list_of_blindset_ids_only\"\n",
    "    blind_set_path = sys.argv[2]\n",
    "    for ID in ids:\n",
    "        write_upper(os.path.join(blind_set_path, ID+\".fasta\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reran!\n",
    "--> No lowercase letters in my fasta blindset\n",
    "Updated script and fixed all files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking % of E, C, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first I want to read line 2 of each file \n",
    "# and save the string\n",
    "\n",
    "def count_C_H_E(infile1):                                              # call list of file names and for dsspfile\n",
    "    ''' Reads line 2 (index 1) from a fastalike dssp file. Returns a tuple \n",
    "    containing the number of C, H, E in that order. '''\n",
    "    cleanlines_string = ''\n",
    "    C=0\n",
    "    H=0\n",
    "    E=0\n",
    "    with open(infile1) as ofile:\n",
    "        flist = ofile.readlines()[1:] # list of each line of the file excluding line 0 \n",
    "        nonewline = flist[0].rstrip()\n",
    "        for character in nonewline:\n",
    "            if character == \"-\":\n",
    "                C+=1\n",
    "            elif character == 'H':\n",
    "                H+=1\n",
    "            elif character == 'E':\n",
    "                E+=1\n",
    "            else:\n",
    "                print(\"Unknown character!!!\", character)\n",
    "        return C, H, E    # returns number of C H and E\n",
    "    \n",
    "# a = lines_list(\"./blind_dssp/4ywn.dssp\")\n",
    "# print(a)\n",
    "\n",
    "def stitchingstrings(looplist):\n",
    "    all_list = []\n",
    "    sumC = 0           #defining final sums\n",
    "    sumH = 0\n",
    "    sumE = 0\n",
    "    totchar=0\n",
    "    percC=0\n",
    "    percH=0\n",
    "    percE=0\n",
    "    with open(looplist, 'r') as filenames:\n",
    "        all_list = filenames.readlines()  # rading file ids into list\n",
    "    for el in all_list:                   # for each filename\n",
    "        filename = el.rstrip()\n",
    "        C, H, E = count_C_H_E('trainingset/dssp/'+filename)         #  call function count_C_H_E with current filename\n",
    "        sumC += C                         # increment previous sum by each letter count\n",
    "        sumH += H\n",
    "        sumE += E\n",
    "    totchar += sumC + sumH + sumE\n",
    "    percC = sumC/totchar*100\n",
    "    percH = sumH/totchar*100\n",
    "    percE = sumE/totchar*100\n",
    "    return percC, percH, percE\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ila/01-Unibo/02_Lab2/files_lab2_project'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.91366317169069 37.68020969855832 24.40612712975098\n"
     ]
    }
   ],
   "source": [
    "C, H, E = stitchingstrings(\"dssp_filenames\")\n",
    "print(C, H, E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The blind_set contains\n",
    "\n",
    "37.91% C\n",
    "\n",
    "37.68% H\n",
    "\n",
    "24,41% E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.16215473786861 35.592731468128065 22.245113794003323\n"
     ]
    }
   ],
   "source": [
    "C, H, E = stitchingstrings(\"dssp_filenames_trainingset\")\n",
    "print(C, H, E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training_set contains\n",
    "\n",
    "42.16% C\n",
    "\n",
    "35.59% H\n",
    "\n",
    "22.24% E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Set | training | blind | \n",
    "|-----|----------|-------|\n",
    "| C   |42.16%|37.91%|\n",
    "| H   |35.59%|37.68%|\n",
    "| E   |22.24%|24,41%|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling back\n",
    "\n",
    "'X' dont seem to be mapped to 'C'\n",
    "\n",
    "Rerunning script on subset from\n",
    "\n",
    "    /Users/ila/01-Unibo/02_Lab2/files_lab2_project/all_data/blindset/blindset_all_PDBs/150_blind_PDBs/unused_10/dsspout_unused10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Found some X that were not mapped to coil:\n",
    "\n",
    "* Parsing my files I noticed that not all X in the fasta were assigned to coils in the DSSP fasta-like file. So I checked the original DSSP output files (assuming something was wrong with my script). But I found that some DSSP files generated from the PDB files contain X already but are mapped to different SS structures such as H here in the example.\n",
    "E.g. 4zey chain A See line 2:\n",
    "In this case we obviously retain the SS (even if it is not assigned to Coil) computed by the DSSP program which took the structural information from the PDB file right?\n",
    "\n",
    "A: Prof Savojardo Yes, you can retain the SS assigned by DSSP. This happens because DSSP is able to assign the structure even if you have an X, probably based on the context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Secondary Structure Definition by the program DSSP, CMBI version by M.L. Hekkelman/2010-10-21 ==== DATE=2020-09-26        .\n",
      "REFERENCE W. KABSCH AND C.SANDER, BIOPOLYMERS 22 (1983) 2577-2637                                                              .\n",
      "HEADER    TRANSCRIPTION REGULATOR                 20-APR-15   4ZEY                                                             .\n",
      "COMPND    MOL_ID: 1; MOLECULE: NUCLEAR RECEPTOR-BINDING FACTOR 2; CHAIN: A; FRAG                                               .\n",
      "SOURCE    MOL_ID: 1; ORGANISM_SCIENTIFIC: HOMO SAPIENS; ORGANISM_COMMON: HUMAN;                                                .\n",
      "AUTHOR    JOINT CENTER FOR STRUCTURAL GENOMICS (JCSG),PARTNERSHIP FOR NUCLEAR RE                                               .\n",
      "   84  1  0  0  0 TOTAL NUMBER OF RESIDUES, NUMBER OF CHAINS, NUMBER OF SS-BRIDGES(TOTAL,INTRACHAIN,INTERCHAIN)                .\n",
      "  6386.7   ACCESSIBLE SURFACE OF PROTEIN (ANGSTROM**2)                                                                         .\n",
      "   72 85.7   TOTAL NUMBER OF HYDROGEN BONDS OF TYPE O(I)-->H-N(J)  , SAME NUMBER PER 100 RESIDUES                              .\n",
      "    0  0.0   TOTAL NUMBER OF HYDROGEN BONDS IN     PARALLEL BRIDGES, SAME NUMBER PER 100 RESIDUES                              .\n",
      "    0  0.0   TOTAL NUMBER OF HYDROGEN BONDS IN ANTIPARALLEL BRIDGES, SAME NUMBER PER 100 RESIDUES                              .\n",
      "    0  0.0   TOTAL NUMBER OF HYDROGEN BONDS OF TYPE O(I)-->H-N(I-5), SAME NUMBER PER 100 RESIDUES                              .\n",
      "    0  0.0   TOTAL NUMBER OF HYDROGEN BONDS OF TYPE O(I)-->H-N(I-4), SAME NUMBER PER 100 RESIDUES                              .\n",
      "    0  0.0   TOTAL NUMBER OF HYDROGEN BONDS OF TYPE O(I)-->H-N(I-3), SAME NUMBER PER 100 RESIDUES                              .\n",
      "    0  0.0   TOTAL NUMBER OF HYDROGEN BONDS OF TYPE O(I)-->H-N(I-2), SAME NUMBER PER 100 RESIDUES                              .\n",
      "    0  0.0   TOTAL NUMBER OF HYDROGEN BONDS OF TYPE O(I)-->H-N(I-1), SAME NUMBER PER 100 RESIDUES                              .\n",
      "    0  0.0   TOTAL NUMBER OF HYDROGEN BONDS OF TYPE O(I)-->H-N(I+0), SAME NUMBER PER 100 RESIDUES                              .\n",
      "    0  0.0   TOTAL NUMBER OF HYDROGEN BONDS OF TYPE O(I)-->H-N(I+1), SAME NUMBER PER 100 RESIDUES                              .\n",
      "    1  1.2   TOTAL NUMBER OF HYDROGEN BONDS OF TYPE O(I)-->H-N(I+2), SAME NUMBER PER 100 RESIDUES                              .\n",
      "    5  6.0   TOTAL NUMBER OF HYDROGEN BONDS OF TYPE O(I)-->H-N(I+3), SAME NUMBER PER 100 RESIDUES                              .\n",
      "   65 77.4   TOTAL NUMBER OF HYDROGEN BONDS OF TYPE O(I)-->H-N(I+4), SAME NUMBER PER 100 RESIDUES                              .\n",
      "    1  1.2   TOTAL NUMBER OF HYDROGEN BONDS OF TYPE O(I)-->H-N(I+5), SAME NUMBER PER 100 RESIDUES                              .\n",
      "  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30     *** HISTOGRAMS OF ***           .\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  1  0  0  0  0  0  0  0  0  0  0  1    RESIDUES PER ALPHA HELIX         .\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0    PARALLEL BRIDGES PER LADDER      .\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0    ANTIPARALLEL BRIDGES PER LADDER  .\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0    LADDERS PER SHEET                .\n",
      "  #  RESIDUE AA STRUCTURE BP1 BP2  ACC     N-H-->O    O-->H-N    N-H-->O    O-->H-N    TCO  KAPPA ALPHA  PHI   PSI    X-CA   Y-CA   Z-CA \n",
      "    1    0 A G              0   0  115      0, 0.0     0, 0.0     0, 0.0     0, 0.0   0.000 360.0 360.0 360.0-140.1    5.6   25.9    3.8\n",
      "    2    4 A X        -     0   0  193      1,-0.1     2,-0.4     5,-0.0     5,-0.0   0.114 360.0-152.1 -37.7 144.1    8.5   23.8    5.4\n",
      "    3    5 A E        +     0   0  113      4,-0.1    -1,-0.1     3,-0.0     0, 0.0  -0.983  15.8 177.2-135.2 120.8    9.3   20.3    4.1\n",
      "    4    6 A G     >  -     0   0   40     -2,-0.4     4,-2.3    38,-0.0     5,-0.1  -0.448  52.8 -87.2-100.7-179.8   10.8   17.4    6.0\n",
      "    5    7 A P  H  > S+     0   0   30      0, 0.0     4,-2.2     0, 0.0     5,-0.1   0.858 127.4  52.0 -67.6 -33.3   11.3   13.9    4.4\n",
      "    6    8 A L  H  > S+     0   0   62      2,-0.2     4,-2.4     1,-0.2     5,-0.2   0.942 110.6  49.7 -65.7 -41.8    7.8   12.6    5.1\n",
      "    7    9 A N  H  > S+     0   0   69      1,-0.2     4,-2.3     2,-0.2    -1,-0.2   0.921 112.0  48.9 -59.9 -44.4    6.3   15.8    3.5\n",
      "    8   10 A L  H  X S+     0   0   38     -4,-2.3     4,-2.4     2,-0.2    -1,-0.2   0.882 108.4  53.0 -65.7 -34.4    8.5   15.2    0.5\n",
      "    9   11 A A  H  X S+     0   0    0     -4,-2.2     4,-2.0     2,-0.2    -2,-0.2   0.952 111.3  46.5 -64.0 -43.4    7.5   11.5    0.2\n",
      "   10   12 A H  H  X S+     0   0   77     -4,-2.4     4,-2.2     1,-0.2    -2,-0.2   0.878 110.6  53.3 -65.5 -39.7    3.8   12.5    0.2\n",
      "   11   13 A Q  H  X S+     0   0   66     -4,-2.3     4,-2.2     1,-0.2    -1,-0.2   0.893 110.0  47.5 -62.4 -44.7    4.6   15.3   -2.4\n",
      "   12   14 A Q  H  X S+     0   0   26     -4,-2.4     4,-2.2     2,-0.2    -1,-0.2   0.853 109.3  54.0 -65.2 -35.0    6.2   12.7   -4.7\n",
      "   13   15 A S  H  X S+     0   0   31     -4,-2.0     4,-1.4     2,-0.2    -2,-0.2   0.940 109.7  47.1 -65.2 -40.6    3.3   10.3   -4.2\n",
      "   14   16 A R  H  X S+     0   0  129     -4,-2.2     4,-1.4     1,-0.2     3,-0.2   0.921 112.6  49.6 -63.3 -44.2    0.8   13.1   -5.3\n",
      "   15   17 A R  H  X S+     0   0  118     -4,-2.2     4,-2.6     1,-0.2    -1,-0.2   0.875 104.5  59.6 -63.4 -37.5    3.0   13.9   -8.3\n",
      "   16   18 A A  H  X S+     0   0    0     -4,-2.2     4,-2.7     1,-0.2    -1,-0.2   0.888 103.8  50.2 -61.1 -40.4    3.2   10.2   -9.3\n",
      "   17   19 A D  H  X S+     0   0   95     -4,-1.4     4,-2.2    -3,-0.2    -1,-0.2   0.903 111.4  49.4 -63.0 -44.3   -0.7   10.0   -9.7\n",
      "   18   20 A R  H  X S+     0   0  156     -4,-1.4     4,-1.3     1,-0.2    -2,-0.2   0.930 114.2  44.6 -59.3 -50.6   -0.6   13.1  -11.9\n",
      "   19   21 A L  H  <>S+     0   0    7     -4,-2.6     5,-2.7     1,-0.2     3,-0.3   0.904 112.2  50.8 -64.8 -41.6    2.1   11.8  -14.1\n",
      "   20   22 A L  H ><5S+     0   0   50     -4,-2.7     3,-1.6     1,-0.2    -1,-0.2   0.896 107.5  53.5 -64.4 -39.5    0.6    8.2  -14.4\n",
      "   21   23 A A  H 3<5S+     0   0   94     -4,-2.2    -1,-0.2     1,-0.3    -2,-0.2   0.798 107.6  53.1 -62.8 -29.5   -2.8    9.7  -15.5\n",
      "   22   24 A A  T 3<5S-     0   0   71     -4,-1.3    -1,-0.3    -3,-0.3    -2,-0.2   0.452 122.2-109.1 -83.3  -2.2   -0.9   11.6  -18.2\n",
      "   23   25 A G  T < 5S+     0   0   42     -3,-1.6     2,-1.4    -4,-0.2    -3,-0.2   0.605  75.2 139.3  81.3  17.7    0.7    8.3  -19.5\n",
      "   24   26 A K     >< +     0   0  100     -5,-2.7     4,-2.0     1,-0.2    -1,-0.2  -0.626  19.6 165.3 -95.5  75.5    4.2    9.3  -18.1\n",
      "   25   27 A Y  H  >  +     0   0   53     -2,-1.4     4,-2.4     2,-0.2    -1,-0.2   0.905  69.8  47.4 -60.8 -50.1    5.1    5.8  -16.8\n",
      "   26   28 A E  H  > S+     0   0  123      1,-0.2     4,-2.4     2,-0.2    -1,-0.2   0.913 113.3  48.7 -61.8 -44.4    8.8    6.3  -16.3\n",
      "   27   29 A E  H  > S+     0   0   97      1,-0.2     4,-2.1     2,-0.2    -1,-0.2   0.853 111.2  51.6 -63.7 -38.6    8.3    9.6  -14.5\n",
      "   28   30 A A  H  X S+     0   0    0     -4,-2.0     4,-1.8     2,-0.2    -1,-0.2   0.910 109.2  49.3 -66.4 -40.3    5.6    8.0  -12.3\n",
      "   29   31 A I  H  X S+     0   0    9     -4,-2.4     4,-2.3     2,-0.2    -2,-0.2   0.935 109.3  53.0 -62.9 -43.1    8.0    5.1  -11.3\n",
      "   30   32 A S  H  X S+     0   0   43     -4,-2.4     4,-2.5     1,-0.2     5,-0.2   0.917 107.6  51.4 -59.3 -41.3   10.7    7.6  -10.5\n",
      "   31   33 A C  H  X S+     0   0    7     -4,-2.1     4,-2.7     1,-0.2    -1,-0.2   0.853 110.3  48.6 -63.5 -40.5    8.3    9.5   -8.1\n",
      "   32   34 A H  H  X S+     0   0    2     -4,-1.8     4,-1.9     2,-0.2    -1,-0.2   0.868 112.4  47.8 -68.4 -33.7    7.4    6.2   -6.3\n",
      "   33   35 A K  H  X S+     0   0  122     -4,-2.3     4,-1.7     2,-0.2    -2,-0.2   0.860 113.8  48.1 -71.2 -36.3   11.1    5.3   -6.0\n",
      "   34   36 A K  H  X S+     0   0  104     -4,-2.5     4,-2.6     2,-0.2     5,-0.2   0.910 109.8  52.5 -69.1 -42.8   11.8    8.9   -4.6\n",
      "   35   37 A A  H  X S+     0   0    0     -4,-2.7     4,-2.1     1,-0.2    -2,-0.2   0.923 108.9  50.0 -59.3 -39.5    8.9    8.6   -2.3\n",
      "   36   38 A A  H  X S+     0   0   10     -4,-1.9     4,-2.0     2,-0.2    -1,-0.2   0.896 110.5  50.4 -65.6 -36.5   10.3    5.3   -0.9\n",
      "   37   39 A A  H  X S+     0   0   54     -4,-1.7     4,-1.6     1,-0.2    -2,-0.2   0.923 111.2  47.3 -67.1 -44.1   13.8    7.0   -0.4\n",
      "   38   40 A Y  H  X S+     0   0   65     -4,-2.6     4,-2.8     1,-0.2    -1,-0.2   0.837 110.4  53.0 -66.1 -36.0   12.2    9.9    1.5\n",
      "   39   41 A L  H  X S+     0   0    0     -4,-2.1     4,-2.5    -5,-0.2    -1,-0.2   0.884 104.3  55.6 -65.7 -37.5   10.2    7.4    3.6\n",
      "   40   42 A S  H  X S+     0   0   37     -4,-2.0     4,-0.6     2,-0.2    -2,-0.2   0.896 111.3  45.6 -57.8 -38.4   13.5    5.6    4.4\n",
      "   41   43 A E  H >< S+     0   0  104     -4,-1.6     3,-1.3     2,-0.2     4,-0.5   0.949 111.9  49.7 -69.1 -49.7   14.8    9.0    5.7\n",
      "   42   44 A A  H >< S+     0   0    6     -4,-2.8     3,-1.5     1,-0.3     4,-0.4   0.847 102.8  63.2 -60.4 -31.8   11.5    9.8    7.7\n",
      "   43   45 A X  H >< S+     0   0   15     -4,-2.5     3,-0.7     1,-0.3    -1,-0.3   0.772  94.8  60.4 -64.0 -25.1   11.7    6.2    9.3\n",
      "   44   46 A K  T << S+     0   0  176     -3,-1.3    -1,-0.3    -4,-0.6    -2,-0.2   0.702 104.8  50.6 -74.3 -16.1   15.0    7.2   10.9\n",
      "   45   47 A L  T <  S+     0   0  122     -3,-1.5    -1,-0.2    -4,-0.5    -2,-0.2   0.432  81.6 117.5-101.3  -3.8   13.1   10.1   12.8\n",
      "   46   48 A T    <   -     0   0   38     -3,-0.7     3,-0.1    -4,-0.4    -3,-0.0  -0.306  41.2-171.0 -67.2 150.0   10.2    8.0   14.2\n",
      "   47   49 A Q        +     0   0  203      1,-0.1     2,-0.3     0, 0.0    -1,-0.1   0.361  67.7  56.4-118.3  -6.6    9.9    7.9   18.0\n",
      "   48   50 A S  S  > S-     0   0   35      1,-0.1     4,-2.2     0, 0.0     5,-0.1  -0.969  74.3-132.2-130.2 150.4    7.2    5.2   18.2\n",
      "   49   51 A E  H  > S+     0   0  122     -2,-0.3     4,-3.1     2,-0.2     5,-0.2   0.868 109.0  55.9 -66.8 -33.3    6.9    1.7   16.9\n",
      "   50   52 A Q  H  > S+     0   0  142      2,-0.2     4,-2.2     1,-0.2    -1,-0.2   0.933 109.9  44.8 -68.3 -39.4    3.4    2.4   15.4\n",
      "   51   53 A A  H  > S+     0   0   38      2,-0.2     4,-2.5     1,-0.2    -2,-0.2   0.931 115.7  48.4 -67.8 -41.2    4.7    5.4   13.4\n",
      "   52   54 A H  H  X S+     0   0   56     -4,-2.2     4,-2.4     2,-0.2    -2,-0.2   0.940 112.5  46.7 -62.2 -48.8    7.7    3.3   12.2\n",
      "   53   55 A L  H  X S+     0   0   83     -4,-3.1     4,-2.1     1,-0.2    -1,-0.2   0.881 113.4  50.0 -62.0 -40.4    5.6    0.3   11.2\n",
      "   54   56 A S  H  X S+     0   0   73     -4,-2.2     4,-2.3    -5,-0.2    -1,-0.2   0.895 110.4  48.3 -68.4 -36.5    3.1    2.5    9.4\n",
      "   55   57 A L  H  X S+     0   0   16     -4,-2.5     4,-2.5     2,-0.2    -2,-0.2   0.911 110.5  53.2 -71.2 -33.9    5.9    4.4    7.4\n",
      "   56   58 A E  H  X S+     0   0   87     -4,-2.4     4,-2.5     2,-0.2    -2,-0.2   0.906 109.2  47.5 -63.9 -42.6    7.4    1.0    6.5\n",
      "   57   59 A L  H  X S+     0   0  106     -4,-2.1     4,-2.3     2,-0.2    -1,-0.2   0.931 111.0  53.0 -65.8 -37.6    4.0   -0.2    5.1\n",
      "   58   60 A Q  H  X S+     0   0   42     -4,-2.3     4,-1.7     2,-0.2    -2,-0.2   0.879 109.2  49.2 -61.2 -42.3    3.8    3.1    3.2\n",
      "   59   61 A R  H  X S+     0   0   73     -4,-2.5     4,-1.2     1,-0.2     3,-0.3   0.947 110.6  49.4 -66.0 -43.1    7.2    2.5    1.7\n",
      "   60   62 A D  H  X S+     0   0   75     -4,-2.5     4,-2.1     1,-0.2     3,-0.3   0.868 104.9  59.3 -63.5 -32.8    6.2   -1.1    0.7\n",
      "   61   63 A S  H  X S+     0   0   53     -4,-2.3     4,-2.2     1,-0.2    -1,-0.2   0.923 102.0  53.2 -62.8 -36.7    3.0    0.3   -0.9\n",
      "   62   64 A H  H  X S+     0   0    9     -4,-1.7     4,-2.2    -3,-0.3    -1,-0.2   0.832 107.3  52.7 -66.1 -31.5    5.2    2.5   -3.2\n",
      "   63   65 A X  H  X S+     0   0   63     -4,-1.2     4,-2.0    -3,-0.3    -1,-0.2   0.880 107.9  49.8 -68.4 -38.8    7.0   -0.8   -4.2\n",
      "   64   66 A K  H  X S+     0   0  120     -4,-2.1     4,-2.3     2,-0.2    -2,-0.2   0.899 110.4  51.2 -66.8 -37.0    3.7   -2.4   -5.1\n",
      "   65   67 A Q  H  X S+     0   0   71     -4,-2.2     4,-3.2     2,-0.2     5,-0.3   0.914 106.9  53.6 -62.5 -44.3    2.9    0.7   -7.1\n",
      "   66   68 A L  H  X S+     0   0   37     -4,-2.2     4,-2.7     1,-0.2    -2,-0.2   0.926 109.9  47.7 -59.0 -43.0    6.3    0.4   -8.9\n",
      "   67   69 A L  H  X S+     0   0  121     -4,-2.0     4,-2.2     2,-0.2    -2,-0.2   0.936 114.5  45.9 -65.6 -43.2    5.4   -3.3   -9.9\n",
      "   68   70 A L  H  X S+     0   0   91     -4,-2.3     4,-2.4     1,-0.2    -2,-0.2   0.939 114.8  46.6 -62.8 -48.2    1.9   -2.4  -11.1\n",
      "   69   71 A I  H  X S+     0   0    1     -4,-3.2     4,-2.7     1,-0.2    -1,-0.2   0.866 109.9  53.8 -66.8 -39.1    3.2    0.6  -13.1\n",
      "   70   72 A Q  H  X S+     0   0   88     -4,-2.7     4,-2.2    -5,-0.3    -1,-0.2   0.948 110.8  46.7 -59.6 -46.0    6.1   -1.4  -14.6\n",
      "   71   73 A E  H  X S+     0   0  106     -4,-2.2     4,-1.9     1,-0.2    -2,-0.2   0.912 114.7  46.5 -60.8 -45.6    3.6   -4.0  -15.9\n",
      "   72   74 A R  H  X S+     0   0   99     -4,-2.4     4,-2.3     1,-0.2    -1,-0.2   0.855 110.0  54.2 -66.0 -34.0    1.2   -1.4  -17.2\n",
      "   73   75 A W  H  X S+     0   0   53     -4,-2.7     4,-2.7     2,-0.2    -2,-0.2   0.910 106.6  50.9 -66.0 -44.7    4.1    0.5  -18.9\n",
      "   74   76 A K  H  X S+     0   0  138     -4,-2.2     4,-2.3     2,-0.2    -2,-0.2   0.933 111.2  49.0 -57.1 -47.1    5.2   -2.7  -20.7\n",
      "   75   77 A R  H  X S+     0   0  151     -4,-1.9     4,-2.2     1,-0.2    -2,-0.2   0.918 112.7  47.4 -58.1 -44.7    1.6   -3.2  -22.0\n",
      "   76   78 A A  H  X S+     0   0   24     -4,-2.3     4,-2.5     1,-0.2    -2,-0.2   0.855 110.1  53.0 -68.1 -36.2    1.5    0.5  -23.2\n",
      "   77   79 A Q  H  X S+     0   0   97     -4,-2.7     4,-1.8     2,-0.2    -1,-0.2   0.912 110.4  46.7 -64.0 -44.2    4.9    0.2  -24.9\n",
      "   78   80 A R  H  X S+     0   0  164     -4,-2.3     4,-2.2     2,-0.2    -2,-0.2   0.905 113.0  50.0 -64.6 -41.1    3.6   -2.9  -26.9\n",
      "   79   81 A E  H  X S+     0   0   87     -4,-2.2     4,-2.1     1,-0.2    -2,-0.2   0.895 107.4  54.6 -63.1 -40.4    0.4   -1.1  -27.7\n",
      "   80   82 A E  H  X S+     0   0  144     -4,-2.5     4,-0.6     1,-0.2    -1,-0.2   0.895 111.3  44.2 -59.8 -45.4    2.4    2.0  -29.0\n",
      "   81   83 A R  H >< S+     0   0  178     -4,-1.8     3,-0.5     2,-0.2    -1,-0.2   0.864 111.3  54.3 -67.0 -38.0    4.4   -0.3  -31.3\n",
      "   82   84 A L  H 3< S+     0   0  139     -4,-2.2    -2,-0.2     1,-0.2    -1,-0.2   0.887 113.9  41.1 -63.4 -38.5    1.3   -2.1  -32.5\n",
      "   83   85 A K  H 3<        0   0  155     -4,-2.1    -1,-0.2    -5,-0.1    -2,-0.2   0.443 360.0 360.0 -89.8  -7.6   -0.5    1.2  -33.4\n",
      "   84   86 A A    <<        0   0  142     -4,-0.6    -3,-0.0    -3,-0.5    -4,-0.0  -0.465 360.0 360.0 -77.4 360.0    2.6    3.0  -35.0\n"
     ]
    }
   ],
   "source": [
    "!cat /Users/ila/01-Unibo/02_Lab2/files_lab2_project/ztests_and_other_snippets/deletechainmess/testruns/pdb4zey.ent.dssp"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4ywn:A\n",
      "5fly:A\n",
      "5jvv:A\n",
      "5xjl:2\n",
      "6eha:A\n",
      "6ntv:A\n"
     ]
    }
   ],
   "source": [
    "def lines_list(infile1):                                              # call for list of file names AND for dsspfile\n",
    "    ''' Reads all lines from a file and saves them to a list. '''\n",
    "    nonewline = []\n",
    "    with open(infile1) as ofile:\n",
    "        raw_list = ofile.readlines() # returns list containing each line of the file\n",
    "        for el in raw_list:\n",
    "            nonewline.append(el.rstrip())\n",
    "    return nonewline\n",
    "    \n",
    "all_160 = lines_list(\"/Users/ila/01-Unibo/02_Lab2/files_lab2_project/all_data/blindset/blindset_all_PDBs/150_blind_PDBs/all_160_ids_and_chains_blindset\")    \n",
    "wanted_no_chain = lines_list(\"/Users/ila/01-Unibo/02_Lab2/files_lab2_project/all_data/blindset/blindset_all_PDBs/150_blind_PDBs/unused_10/ids_of_unused_10\")\n",
    "\n",
    "def lower_clean_list(list1):\n",
    "    ''' \n",
    "    Takes id list. Returns list of ids from even valued pos\n",
    "    and chain list for odd valued positions.\n",
    "    '''\n",
    "    names_lower = []\n",
    "    chains_upper = []\n",
    "    list_of_splits = []\n",
    "    for i in list1:\n",
    "        sp_li = i.split(':')\n",
    "        names_lower.append(sp_li[0].lower())\n",
    "        chains_upper.append(sp_li[1]) #.rstrip()\n",
    "    return names_lower, chains_upper\n",
    "\n",
    "def dict_from_2_lists(list1, list2):\n",
    "    '''\n",
    "    Takes 2 lists as input and returns a dictionary where items \n",
    "    from list1 are keys and items from list2 are the values.\n",
    "    '''\n",
    "    keys = list1\n",
    "    values = list2\n",
    "    dictionary = dict(zip(list1, list2))\n",
    "    return dictionary\n",
    "\n",
    "def find_position(dict1, list_no_chains):\n",
    "    for el in list_no_chains:\n",
    "        print(el+':'+dict1[el])\n",
    "\n",
    "# Calling functions\n",
    "my_dictionary = dict_from_2_lists(a,b)\n",
    "find_position(my_dictionary,wanted_no_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottomline - don't need to rerun: There was not mistake in the code! The NON coil assignment to \"X\" residues was due to the DSSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
